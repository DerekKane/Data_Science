Sys.Date
time <- Sys.time
time
sent_df$Date <- Sys.Date()
View(sent_df)
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of tweets") +
opts(title = "Sentiment Analysis of Tweets about [var1]\n(classification by polarity)",
plot.title = theme_text(size=12))
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of tweets") +
opts(title = "Sentiment Analysis of Tweets about ",var1,"\n(classification by polarity)",
plot.title = theme_text(size=12))
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of tweets") +
opts(title = "Sentiment Analysis of Tweets about ",Var1,"\n(classification by polarity)",
plot.title = theme_text(size=12))
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of tweets") +
opts(title = "Sentiment Analysis of Tweets about Thor3 \n(classification by polarity)",
plot.title = theme_text(size=12))
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="number of tweets") +
opts(title = "Sentiment Analysis of Tweets about ",Var1, "\n(classification by polarity)",
plot.title = theme_text(size=12))
write.csv(sent_df, file='C:/Users/Derek/Documents/RPackages/Social Media/Tweets.csv', row.names=F)
library("ROAuth")
library("twitteR")
library("wordcloud")
library("tm")
library("plyr")
library("stringr")
library("ggplot2")
library("sentiment")
library("RColorBrewer")
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
Var1 <- '#ChicagoBulls'
Tweets <- searchTwitter(Var1, n=1000, cainfo="cacert.pem")
Tweets.df <- twListToDF(Tweets)
some_txt = sapply(Tweets, function(x) x$getText())
########################################################################
# Prepare the tweets for text analysis.
# remove retweet entities
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
# remove at people
some_txt = gsub("@\\w+", "", some_txt)
# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt = gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
# define "tolower error handling" function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
########################################################################
# Perform Sentiment Analysis.
# classify emotion
class_emo = classify_emotion(some_txt, algorithm="bayes", prior=1.0)
# get emotion best fit
emotion = class_emo[,7]
# substitute NA's by "unknown"
emotion[is.na(emotion)] = "unknown"
# classify polarity
class_pol = classify_polarity(some_txt, algorithm="bayes")
# get polarity best fit
polarity = class_pol[,4]
########################################################################
# Create a data frame with the results.
sent_df = data.frame(text=some_txt, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
View(Tweets.df)
sent_df$SearchTerm <- Var1
View(sent_df)
class_pol = classify_polarity(some_txt, algorithm="bayes", verbose=TRUE)
View(Tweets.df)
library("ROAuth")
library("twitteR")
library("wordcloud")
library("tm")
library("plyr")
library("stringr")
library("ggplot2")
library("sentiment")
library("RColorBrewer")
# This step can be run first if we have already authenticated our twitter
# account.
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
Var1 <- '#ChicagoBulls'
Tweets <- searchTwitter(Var1, n=50, cainfo="cacert.pem")
Tweets.df <- twListToDF(Tweets)
Tweets.df$SearchTerm <- Var1
View(Tweets.df)
names(Tweets.df)
library(Rfacebook)
library(Rook)
load("fb_oauth")
me <- getUsers("me", token=fb_oauth)
View(me)
View(Tweets.df)
# get the text from the tweets
some_txt = sapply(Tweets, function(x) x$getText())
########################################################################
# Prepare the tweets for text analysis.
# remove retweet entities
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
# remove at people
some_txt = gsub("@\\w+", "", some_txt)
# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt = gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
# define "tolower error handling" function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
########################################################################
# Perform Sentiment Analysis.
# classify emotion
class_emo = classify_emotion(some_txt, algorithm="bayes", prior=1.0)
# get emotion best fit
emotion = class_emo[,7]
# substitute NA's by "unknown"
emotion[is.na(emotion)] = "unknown"
# classify polarity
class_pol = classify_polarity(some_txt, algorithm="bayes")
# get polarity best fit
polarity = class_pol[,4]
########################################################################
# Create a data frame with the results.
sent_df = data.frame(SearchTerm=Var1, created=created, text=some_txt, emotion=emotion,
polarity=polarity, screenName=screenName,
retweetCount=retweetCount, stringsAsFactors=FALSE)
sent_df = data.frame(SearchTerm=Var1, created=created, text=some_txt, emotion=emotion,
polarity=polarity, screenName=screenName, stringsAsFactors=FALSE)
library("ROAuth")
library("twitteR")
library("wordcloud")
library("tm")
library("plyr")
library("stringr")
library("ggplot2")
library("sentiment")
library("RColorBrewer")
# This step can be run first if we have already authenticated our twitter
# account.
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
########################################################################
# Get tweets from twitter.
Var1 <- '#ChicagoBulls'
Tweets <- searchTwitter(Var1, n=50, cainfo="cacert.pem")
Tweets.df <- twListToDF(Tweets)
# Sentiment Analysis
########################################################################
# get the text from the tweets
some_txt = sapply(Tweets, function(x) x$getText())
########################################################################
# Prepare the tweets for text analysis.
# remove retweet entities
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
# remove at people
some_txt = gsub("@\\w+", "", some_txt)
# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt = gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
# define "tolower error handling" function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
########################################################################
# Perform Sentiment Analysis.
# classify emotion
class_emo = classify_emotion(some_txt, algorithm="bayes", prior=1.0)
# get emotion best fit
emotion = class_emo[,7]
# substitute NA's by "unknown"
emotion[is.na(emotion)] = "unknown"
# classify polarity
class_pol = classify_polarity(some_txt, algorithm="bayes")
# get polarity best fit
polarity = class_pol[,4]
########################################################################
# Create a data frame with the results.
sent_df = data.frame(SearchTerm=Var1, created=created, text=some_txt, emotion=emotion,
polarity=polarity, screenName=screenName, stringsAsFactors=FALSE)
View(Tweets.df)
sent_df = data.frame(SearchTerm=Var1, text=some_txt, emotion=emotion,
polarity=polarity, screenName=screenName, stringsAsFactors=FALSE)
sent_df = data.frame(text=some_txt, emotion=emotion,
polarity=polarity, screenName=screenName, stringsAsFactors=FALSE)
sent_df = data.frame(text=some_txt, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
View(sent_df)
sent_df$SearchTerm <- Var1
View(Tweets.df)
sent_df$Date <- Tweets.df$created
View(sent_df)
View(Tweets.df)
sent_df$ScreenName <- Tweets.df$screenName
View(sent_df)
View(Tweets.df)
library(Rfacebook)
library(Rook)
#so if you want to connect to Facebook again you just have to call
load("fb_oauth")
Friends <- getFriends(token=fb_oauth, simplify = FALSE)
View(Friends)
my_friends_info <- getUsers(my_friends$id, token=fb_oauth, private_info=TRUE)
my_friends_info <- getUsers(Friends$id, token=fb_oauth, private_info=TRUE)
View(my_friends_info)
Checkin <-getCheckins(Friends$id, n=10, token=fb_oauth, tags = FALSE)
View(Friends)
Checkin <-getCheckins(user="1075852970", n=10, token=fb_oauth, tags = FALSE)
Checkin <-getCheckins(user="1075852970", n=10, token=fb_oauth, tags = FALSE)
Likes <-getLikes(user="1075852970", n=500, token=fb_oauth)
View(Likes)
Likes <-getLikes(Friends$id, n=500, token=fb_oauth)
NewsFeed <- getNewsfeed(token=fb_oauth, n=500)
View(NewsFeed)
Likes <-getLikes(user="1075852970", n=500, token=fb_oauth)
View(Likes)
Page <- getPage(page="Fox Valley Concert Band", token=fb_oauth, n=100, feed=FALSE)
getPage(page="Fox Valley Concert Band", token=fb_oauth, n=100, feed=FALSE)
Page <- getPage(page="Woodford Reserve", token=fb_oauth, n=100, feed=FALSE)
fb_page <- getPage(page="facebook", token=fb_oauth)
View(fb_page)
fb_page <- getPage(page="Woodford Reserve", token=fb_oauth)
fb_page <- getPage(page="WoodfordReserve", token=fb_oauth)
View(fb_page)
fb_page <- getPage(page="FoxValleyConcertBand", token=fb_oauth)
View(fb_page)
posts <- searchFacebook(string ="ebola", token=fb_oauth, n = 500)
library(Rfacebook)
library(Rook)
library("tm")
library("plyr")
library("class")
load("fb_oauth")
Friends <- getFriends(token=fb_oauth, simplify = FALSE)
Likes <-getLikes(user="1075852970", n=500, token=fb_oauth)
View(Likes)
Likes$User <- "1075852970"
View(Likes)
View(Friends)
NewsFeed <- getNewsfeed(token=fb_oauth, n=500)
View(Friends)
NewsFeed$User <- "123456789"
View(NewsFeed)
Page <- getPage(page="FoxValleyConcertBand", token=fb_oauth)
library("ROAuth")
library("twitteR")
library("wordcloud")
library("tm")
library("plyr")
library("stringr")
library("ggplot2")
library("sentiment")
library("RColorBrewer")
# This step can be run first if we have already authenticated our twitter
# account.
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
########################################################################
# Get tweets from twitter.
Var1 <- '#ChicagoBulls'
Tweets <- searchTwitter(Var1, n=50, cainfo="cacert.pem")
Tweets.df <- twListToDF(Tweets)
# Sentiment Analysis
########################################################################
# get the text from the tweets
some_txt = sapply(Tweets, function(x) x$getText())
########################################################################
# Prepare the tweets for text analysis.
# remove retweet entities
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
# remove at people
some_txt = gsub("@\\w+", "", some_txt)
# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt = gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
# define "tolower error handling" function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
########################################################################
# Perform Sentiment Analysis.
# classify emotion
class_emo = classify_emotion(some_txt, algorithm="bayes", prior=1.0)
# get emotion best fit
emotion = class_emo[,7]
# substitute NA's by "unknown"
emotion[is.na(emotion)] = "unknown"
# classify polarity
class_pol = classify_polarity(some_txt, algorithm="bayes")
# get polarity best fit
polarity = class_pol[,4]
########################################################################
# Create a data frame with the results.
sent_df = data.frame(text=some_txt, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
sent_df$SearchTerm <- Var1
sent_df$Date <- Tweets.df$created
sent_df$ScreenName <- Tweets.df$screenName
########################################################################
write.csv(sent_df, file='C:/Business Intelligence/Twitter/Tweets.csv', row.names=F)
library(Rfacebook)
library(Rook)
library("tm")
library("plyr")
library("class")
#so if you want to connect to Facebook again you just have to call
load("fb_oauth")
########################################################################
Friends <- getFriends(token=fb_oauth, simplify = FALSE)
########################################################################
Likes <-getLikes(user="1075852970", n=500, token=fb_oauth)
Likes$User <- "1075852970"
########################################################################
NewsFeed <- getNewsfeed(token=fb_oauth, n=500)
NewsFeed$User <- "123456789"
########################################################################
Page <- getPage(page="FoxValleyConcertBand", token=fb_oauth)
########################################################################
write.csv(Page, file='C:/Business Intelligence/Facebook/Page.csv', row.names=F)
write.csv(NewsFeed, file='C:/Business Intelligence/Facebook/NewsFeed.csv', row.names=F)
write.csv(Likes, file='C:/Business Intelligence/Facebook/Likes.csv', row.names=F)
write.csv(Friends, file='C:/Business Intelligence/Facebook/Friends.csv', row.names=F)
library("ROAuth")
library("twitteR")
library("wordcloud")
library("tm")
library("plyr")
library("stringr")
library("ggplot2")
library("sentiment")
library("RColorBrewer")
# This step can be run first if we have already authenticated our twitter
# account.
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
########################################################################
# Get tweets from twitter.
Var1 <- '#Bulls'
Tweets <- searchTwitter(Var1, n=50, cainfo="cacert.pem")
Tweets.df <- twListToDF(Tweets)
y <- c(“abc”, “abcdef”, “a”, “abcd”)
sapply(y, nchar)
y <- c("abc", "abcdef", "a", "abcd")
sapply(y, nchar)
sum <- 1
repeat{
sum <- sum + 2;
print(sum);
if (sum > 11)
break;
}
x<-0;
while (x < 10)
{
x<- x+4;
print (x);
}
library("openNLP")
x=seq(1,5,0.5)
y=3*x^2
plot(x,y)
plot(log(x),log(y))
res <- lm(log(y) ~ log(x))
res
log(3)
res <- lm(log(y) ~ log(x))
res
res <- lm(log(y) ~ log(x))
res
log(3)
x2 <- log(x)
y2 <- log(y)
plot(x2,y2)
df <- rbind(x2,y2)
View(df)
View(df)
primates=read.table(file=file.choose(),header=TRUE)
library("dataset")
library("datasets")
mydata <- cars
View(mydata)
plot(RMR~Weight,data=primates)
x=seq(1,5,0.5)
y=3*2^x
plot(x,y)
step(glm(logmedv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, data = mydata), direction="forward")
setwd("C:/Users/Derek/Documents/RPackages/Regression Tutorial")
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Load the dataset
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
require(MASS)
mydata <- Boston
attach(mydata)
# Load R Libraries for the analysis
library(aod)
library(ggplot2)
library(reshape2)
library(car)
library(corrplot)
mydata$logmedv <- log(mydata$medv)
step(glm(logmedv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, data = mydata), direction="forward")
step(glm(logmedv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, data = mydata), direction="backward")
View(mydata)
step(glm(logmedv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat, data = mydata), direction="both")
reg2 <-lm(logmedv ~ rm + lstat + crim + zn + chas + dis, data = mydata)
summary(reg2)
residualPlots(reg2)
residualPlots(reg2, ~ 1, fitted=TRUE)
avPlots(reg2, id.n=2, id.cex=0.7)
qqPlot(reg2), id.n=3)
qqPlot(reg2, id.n=3)
qqPlot(reg2)
outlierTest(reg2)
influenceIndexPlot(reg2)
influencePlot(reg2)
ncvTest(reg2)
vif(reg2)
