install.packages("ridge")
install.packages("glmnet")
install.packages("lasso2")
install.packages("colorspace")
install.packages("lars")
install.packages("elasticnet")
install.packages("MASS")
library(ridge)
library(glmnet)
library(lasso2)
library(colorspace)
library(lars)
library(elasticnet)
library(MASS)
data(Prostate)
mydata <- Prostate
View(mydata)
y <- as.numeric(Prostate[,9])
x <- as.matrix(Prostate[,1:8])
select(lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,1,0.001)))
select(lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,10,0.1)))
ridge.train <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,10,0.1))
plot(seq(0,10,0.1), ridge.train$GCV, main="GCV of Ridge Regression", type="l",
xlab=expression(lambda), ylab="GCV")
lambda.ridge <- seq(0,10,0.1)[which.min(ridge.train$GCV)]
abline(v=lambda.ridge, lty=2, col="red")
colors <- rainbow_hcl(8, c = 65, l = 65) # 8 is the number or ind. variables
plot.new()
matplot(seq(0,10,0.1), coef(ridge.train)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline(v=lambda.ridge, lty=2)
text(rep(10, 9), coef(ridge.train)[length(seq(0,10,0.1)),-1], colnames(mydata)[-9], pos=4, col=colors)
threshold <- seq(0,1000,0.1)
ridge.train2 <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = threshold)
colors <- rainbow_hcl(8, c = 65, l = 65) # 8 is the number or ind. variables
plot.new()
matplot(threshold, coef(ridge.train2)[,-1], xlim=c(0,1005), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline(v=lambda.ridge, lty=2)
text(rep(10, 9), coef(ridge.train2)[length(threshold),-1], colnames(mydata)[-9], pos=4, col=colors)
ridge.reg <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = lambda.ridge)
ridge.reg
pred.ridge <- coef(ridge.reg)[1] + coef(ridge.reg)[2]*mydata[,1] +
coef(ridge.reg)[3]*mydata[,2] + coef(ridge.reg)[4]*mydata[,3] +
coef(ridge.reg)[5]*mydata[,4] + coef(ridge.reg)[6]*mydata[,5] +
coef(ridge.reg)[7]*mydata[,6] + coef(ridge.reg)[8]*mydata[,7] +
coef(ridge.reg)[9]*mydata[,8]
mydata$YHat <- coef(ridge.reg)[1] + coef(ridge.reg)[2]*mydata[,1] +
coef(ridge.reg)[3]*mydata[,2] + coef(ridge.reg)[4]*mydata[,3] +
coef(ridge.reg)[5]*mydata[,4] + coef(ridge.reg)[6]*mydata[,5] +
coef(ridge.reg)[7]*mydata[,6] + coef(ridge.reg)[8]*mydata[,7] +
coef(ridge.reg)[9]*mydata[,8]
MSE.Ridge<-(sum((mydata$lpsa-pred.ridge)^2))/nrow(mydata)
# RMSE.Ridge<-sqrt((sum((mydata$lpsa-pred.ridge)^2))/nrow(mydata))
MSE.Ridge
lassoreg.cv <- cv.glmnet(x,y, alpha=1)
plot(lassoreg.cv)
lassoreg.cv$lambda.min
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
plot(lassofits)
plot(lassofits, xvar = "lambda", label = TRUE)
plot(lassofits, xvar = "dev", label = TRUE)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
# mydata$yhat <- predict(lassof
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
plot(lassofits)
plot(lassofits, xvar = "lambda", label = TRUE)
plot(lassofits, xvar = "dev", label = TRUE)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (lassopred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
elasticreg.cv <- cv.glmnet(x,y, alpha=0.2)
plot(elasticreg.cv)
elasticreg.cv$lambda.min
elasticreg.cv$lambda.min
elasticfits <- glmnet(x,y,alpha=0.2, nlambda=100)
plot(elasticfits)
plot(elasticfits, xvar = "lambda", label = TRUE)
plot(elasticfits, xvar = "dev", label = TRUE)
elasticpred <- predict(elasticfits,x,s= elasticreg.cv$lambda.min)
elasticcoef <- predict(elasticfits,x,s=elasticreg.cv$lambda.min, type="coefficients")
elasticcoef
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (elasticpred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
ridge.linreg<- lm(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata)
summary(ridge.linreg)
ridge.linreg2<- lm(lpsa~ lcavol + lweight + svi,
data=mydata)
summary(ridge.linreg2)
linearpred <- predict(ridge.linreg2, newdata = mydata, type = "response")
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (linearpred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
library(ridge)
library(glmnet)
library(lasso2)
library(colorspace)
library(lars)
library(elasticnet)
library(MASS)
data(Prostate)
mydata <- Prostate
y <- as.numeric(Prostate[,9])
x <- as.matrix(Prostate[,1:8])
select(lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,1,0.001)))
lassoreg.cv <- cv.glmnet(x,y, alpha=1)
lassoreg.cv$lambda.min
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (lassopred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
lassofits <- glmnet(x,y,alpha=1, nlambda=1000)
plot(lassofits)
plot(lassofits, xvar = "lambda", label = TRUE)
plot(lassofits, xvar = "dev", label = TRUE)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
library(glmnet)
library(lasso2)
library(colorspace)
library(lars)
library(elasticnet)
data(Prostate)
mydata <- Prostate
y <- as.numeric(mydata[,9])
x <- as.matrix(mydata[,1:8])
model.lasso <- lars(x, y, type="lasso")
lambda.lasso <- c(model.lasso$lambda,0)
beta <- coef(model.lasso)
colors <- rainbow_hcl(8, c = 65, l = 65) # 8 is the number or ind. variables
matplot(lambda.lasso, beta, xlim=c(8,-2), type="o", pch=20, xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors)
text(rep(-0, 9), beta[9,], colnames(x), pos=4, col=colors)
abline(v=lambda.lasso[4], lty=2)
abline(h=0, lty=2)
beta.scale <- attr(model.lasso$beta, "scaled:scale")
beta.rescaled <- beta
for(j in 1:9){
beta.rescaled[j,] <- beta.rescaled[j,]*beta.scale
}
matplot(lambda.lasso, beta.rescaled, xlim=c(8,-2), type="o", pch=20, xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors)
text(rep(-0, 9), beta.rescaled[9,], colnames(x), pos=4, col=colors)
abline(v=lambda.lasso[4], lty=2)
abline(h=0, lty=2)
lassoreg.cv <- cv.glmnet(x,y, alpha=1)
plot(lassoreg.cv)
lassoreg.cv$lambda.min
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
plot(lassofits)
plot(lassofits, xvar = "lambda", label = TRUE)
plot(lassofits, xvar = "dev", label = TRUE)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
set.seed(123)
library(ridge)
library(glmnet)
library(lasso2)
library(colorspace)
library(lars)
library(elasticnet)
library(MASS)
data(Prostate)
mydata <- Prostate
y <- as.numeric(Prostate[,9])
x <- as.matrix(Prostate[,1:8])
select(lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,1,0.001)))
select(lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,10,0.1)))
ridge.train <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,10,0.1))
plot(seq(0,10,0.1), ridge.train$GCV, main="GCV of Ridge Regression", type="l",
xlab=expression(lambda), ylab="GCV")
lambda.ridge <- seq(0,10,0.1)[which.min(ridge.train$GCV)]
abline(v=lambda.ridge, lty=2, col="red")
colors <- rainbow_hcl(8, c = 65, l = 65) # 8 is the number or ind. variables
plot.new()
matplot(seq(0,10,0.1), coef(ridge.train)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline(v=lambda.ridge, lty=2)
text(rep(10, 9), coef(ridge.train)[length(seq(0,10,0.1)),-1], colnames(mydata)[-9], pos=4, col=colors)
threshold <- seq(0,1000,0.1)
ridge.train2 <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = threshold)
colors <- rainbow_hcl(8, c = 65, l = 65) # 8 is the number or ind. variables
plot.new()
matplot(threshold, coef(ridge.train2)[,-1], xlim=c(0,1005), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline(v=lambda.ridge, lty=2)
text(rep(10, 9), coef(ridge.train2)[length(threshold),-1], colnames(mydata)[-9], pos=4, col=colors)
ridge.reg <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = lambda.ridge)
# Printing the ridge-regression coefficient estimates for this problem:
ridge.reg
pred.ridge <- coef(ridge.reg)[1] + coef(ridge.reg)[2]*mydata[,1] +
coef(ridge.reg)[3]*mydata[,2] + coef(ridge.reg)[4]*mydata[,3] +
coef(ridge.reg)[5]*mydata[,4] + coef(ridge.reg)[6]*mydata[,5] +
coef(ridge.reg)[7]*mydata[,6] + coef(ridge.reg)[8]*mydata[,7] +
coef(ridge.reg)[9]*mydata[,8]
mydata$YHat <- coef(ridge.reg)[1] + coef(ridge.reg)[2]*mydata[,1] +
coef(ridge.reg)[3]*mydata[,2] + coef(ridge.reg)[4]*mydata[,3] +
coef(ridge.reg)[5]*mydata[,4] + coef(ridge.reg)[6]*mydata[,5] +
coef(ridge.reg)[7]*mydata[,6] + coef(ridge.reg)[8]*mydata[,7] +
coef(ridge.reg)[9]*mydata[,8]
MSE.Ridge<-(sum((mydata$lpsa-pred.ridge)^2))/nrow(mydata)
# RMSE.Ridge<-sqrt((sum((mydata$lpsa-pred.ridge)^2))/nrow(mydata))
MSE.Ridge
lassoreg.cv <- cv.glmnet(x,y, alpha=1)
plot(lassoreg.cv)
lassoreg.cv$lambda.min
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
plot(lassofits)
plot(lassofits, xvar = "lambda", label = TRUE)
plot(lassofits, xvar = "dev", label = TRUE)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (lassopred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
library(neuralnet)
View(mydata)
View(mydata)
mydata$YHat <- NULL
set.seed(1234567890)
Churnnet <- neuralnet(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, mydata, hidden = 4, lifesign = "minimal",
linear.output = FALSE, learningrate = 0.3, threshold = 0.10, algorithm = "backprop", rep=2)
library(neuralnet)
install.packages(neuralnet)
install.packages("neuralnet")
library(neuralnet)
set.seed(1234567890)
Churnnet <- neuralnet(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, mydata, hidden = 4, lifesign = "minimal",
linear.output = FALSE, learningrate = 0.3, threshold = 0.10, algorithm = "backprop", rep=2)
Churnnet <- neuralnet(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, mydata, hidden = 4, lifesign = "minimal",
linear.output = FALSE, learningrate = 0.3, threshold = 0.10, algorithm = "backprop", rep=100)
plot(Churnnet, rep = "best")
NNResult <- compute(Churnnet, mydata[,1:8])
mydata$YHat4 <- NNResult$net.result[1]
View(mydata)
mydata$YHat4 <- NULL
set.seed(1234567890)
Churnnet <- neuralnet(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, mydata, hidden = 4, lifesign = "minimal",
linear.output = FALSE, learningrate = 0.3, threshold = 0.10, rep=100)
NNResult <- compute(Churnnet, mydata[,1:8])
mydata$YHat4 <- NNResult$net.result[1]
mydata
install.packages("caret")
library(caret)
NNet <- avNNet(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = mydata, repeats = 25, size=5, decay=0.3, linout=TRUE)
plot(NNet, rep = "best")
mydata$YHat4 <- predict(NNet, mydata)
mydata
setwd("C:/Users/Derek/Documents/RPackages/Business Metrics/")
setwd("C:/Users/dkane/Documents/R Scripts/Self Organizing Maps/")
library(kohonen)
library(dummies)
library(ggplot2)
library(sp)
library(maptools)
library(reshape2)
library(rgeos)
pretty_palette <- c("#1f77b4", '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2')
small_areas <- TRUE  # Choose between Small Areas or Electoral Districts
filter <- TRUE       # choose to filter output to Dublin area only (good for small areas)
# Load the data into a data frame
# Get the map of these areas and filter for Dublin areas.
if (small_areas){
data_raw <- read.csv("./census_data/AllThemesTablesSA.csv")
ireland_map <- readShapePoly('./boundary_files/Census2011_Small_Areas_generalised20m.shp')
#Note that the map polygons and the census data are not in the same order - rearrangement:
data_raw <- data_raw[match(ireland_map$SMALL_AREA, data_raw$GEOGDESC),]
idcol="GEOGDESC"
} else {
data_raw <- read.csv("./census_data/AllThemesTablesED.csv")
names(data_raw)[1] <- "GEOGID"
ireland_map <- readShapePoly('./boundary_files/Census2011_Electoral_Divisions_generalised20m.shp')
ireland_map$CSOED <- paste0("E", ireland_map$CSOED)
#Note that the map polygons and the census data are not in the same order
data_raw <- data_raw[match(ireland_map$CSOED, data_raw$GEOGID),]
idcol="GEOGID"
}
#Filter now for certain counties
if (filter){
counties <- c("Fingal", "Dublin City", "South Dublin", "DÃºn Laoghaire-Rathdown")
plot_idx <- ireland_map$COUNTYNAME %in% counties
data_raw <- data_raw[plot_idx,]
ireland_map <- ireland_map[plot_idx,]
rm(counties, filter, plot_idx)
}
### -------------- Data processing -------------------------
#convert the data from summations to percentages such
#that the characteristics of each area will be comparable.
source("convertCSOdata.R")
data <- convertCSOdata(data_raw, idcol=idcol)
#Create SOM for Census data - simple as data is well behaved.
#remove incomplete samples:
incompletes <- which(!complete.cases(data))
#where the avr_education_level is NaN - replace with mean
data$avr_education_level[incompletes] <- mean(data$avr_education_level, na.rm=TRUE)
#recalculate after adjustment
incompletes <- which(!complete.cases(data))
if (length(incompletes) > 0){
print(paste0("Removing ", length(incompletes), " data points that have missing values."))
data <- data[-incompletes, ]
}
rm(incompletes)
data_train <- data[, c(2,4,5,8)]
View(data)
View(data_train)
data_train_matrix <- as.matrix(scale(data_train))
names(data_train_matrix) <- names(data_train)
require(kohonen)
if (small_areas){
# larger grid for the small areas example (more samples)
som_grid <- somgrid(xdim = 20, ydim=20, topo="hexagonal")
} else {
som_grid <- somgrid(xdim = 10, ydim=10, topo="hexagonal")
}
# Train the SOM model!
system.time(som_model <- som(data_train_matrix,
grid=som_grid,
rlen=100,
alpha=c(0.05,0.01),
n.hood = "circular",
keep.data = TRUE ))
rm(som_grid, data_train_matrix)
source('coolBlueHotRed.R')
plot(som_model, type = "changes")
#counts within nodes
plot(som_model, type = "counts", main="Node Counts", palette.name=coolBlueHotRed)
#map quality
plot(som_model, type = "quality", main="Node Quality/Distance", palette.name=coolBlueHotRed)
#neighbour distances
plot(som_model, type="dist.neighbours", main = "SOM neighbour distances", palette.name=grey.colors)
#code spread
plot(som_model, type = "codes")
var <- 2
plot(som_model, type = "property", property = som_model$codes[,var], main=names(som_model$data)[var], palette.name=coolBlueHotRed)
# Plot the original scale heatmap for a variable from the training set:
var <- 4 #define the variable to plot
var_unscaled <- aggregate(as.numeric(data_train[,var]), by=list(som_model$unit.classif), FUN=mean, simplify=TRUE)[,2]
plot(som_model, type = "property", property=var_unscaled, main=names(data_train)[var], palette.name=coolBlueHotRed)
rm(var_unscaled, var)
#plot a variable from the original data set (will be uncapped etc.)
# This function produces a menu for multiple heatmaps.
source('plotHeatMap.R')
plotHeatMap(som_model, data, variable=0)
mydata <- som_model$codes
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata,
centers=i)$withinss)
par(mar=c(5.1,4.1,4.1,2.1))
plot(1:15, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares", main="Within cluster sum of squares (WCSS)")
som_cluster <- cutree(hclust(dist(som_model$codes)), 6)
som_cluster
data_train$som_cluster <- cutree(hclust(dist(som_model$codes)), 6)
plot(som_model, type="mapping", bgcol = pretty_palette[som_cluster], main = "Clusters")
add.cluster.boundaries(som_model, som_cluster)
#show the same plot with the codes instead of just colours
plot(som_model, type="codes", bgcol = pretty_palette[som_cluster], main = "Clusters")
add.cluster.boundaries(som_model, som_cluster)
plot(som_model, type="mapping", bgcol = pretty_palette[som_cluster], main = "Clusters")
add.cluster.boundaries(som_model, som_cluster)
#show the same plot with the codes instead of just colours
plot(som_model, type="codes", bgcol = pretty_palette[som_cluster], main = "Clusters")
add.cluster.boundaries(som_model, som_cluster)
cluster_details <- data.frame(id=data$id, cluster=som_cluster[som_model$unit.classif])
View(cluster_details)
View(cluster_details)
View(cluster_details)
som_cluster
plot(som_cluster)
plot(som_cluster, hang= -1)
hc <- hclust(dist(som_model$codes), method="ave")
plot(hc, hang= -1)
rect.hclust(hc, k=6)
groups <- cutree(hc, k=6)
