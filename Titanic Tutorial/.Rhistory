matrix(c(0, 10, 0, 0, 0, 0, 0, 10, 0), ncol = 3)
eval_csc <- evaluate_Weka_classifier(csc, numFolds = 10, complexity = FALSE,seed = 1, class = TRUE)
eval_csc
csc <- CostSensitiveClassifier(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width, data = mydata, control = Weka_control(`cost-matrix` = matrix(c(0,10, 0, 0, 0, 0, 0, 10, 0), ncol = 3), W = "weka.classifiers.trees.J48", M = TRUE))
eval_csc <- evaluate_Weka_classifier(csc, numFolds = 10, complexity = FALSE,seed = 1, class = TRUE)
eval_csc
mydata<-iris
library(randomForest)
set.seed(126)
RandomForestModel <- randomForest(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width, data=mydata, ntree=500, mtry=3, classwt=c(setosa=1,versicolor=1,virginica=1), importance=TRUE)
print(RandomForestModel)
importance(RandomForestModel)
plot.new()
plot(RandomForestModel, log="y")
varImpPlot(RandomForestModel)
plot.new()
varImpPlot(RandomForestModel, type=2, pch=19, col=1, cex=1.0, main="")
mydata$RFPredict <- predict(RandomForestModel, newdata = mydata, type = "response")
View(mydata)
Pred1 <- predict(RandomForestModel, newdata = mydata, type = "prob")
Pred1DF<-data.frame(Pred1)
mydata2<- cbind(mydata, Pred1DF)
View(mydata2)
View(mydata2)
library(xlsx)
write.xlsx(mydata2, "C:/Users/dkane/Documents/RFModelExport.xlsx", sheetName="Sheet1",
col.names=TRUE, row.names=TRUE, append=FALSE, showNA=TRUE)
mydata<-iris
library(randomForest)
set.seed(126)
RandomForestModel <- randomForest(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width, data=mydata, ntree=500, mtry=3, classwt=c(setosa=1,versicolor=5,virginica=1), importance=TRUE)
print(RandomForestModel)
mydata$RFPredict <- predict(RandomForestModel, newdata = mydata, type = "response")
View(mydata)
Pred1 <- predict(RandomForestModel, newdata = mydata, type = "prob")
mydata<-iris
library(randomForest)
#########################################################################
set.seed(126)
# The classwt function is used udring model training to assigning
# a relative cost weight for classification errors. Used to balance the errors.
RandomForestModel <- randomForest(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width, data=mydata, ntree=1, mtry=3, classwt=c(setosa=1,versicolor=5,virginica=1), importance=TRUE)
print(RandomForestModel)
importance(RandomForestModel)
plot.new()
plot(RandomForestModel, log="y")
varImpPlot(RandomForestModel)
plot.new()
varImpPlot(RandomForestModel, type=1, pch=19, col=1, cex=1.0, main="")
abline(v=9, col="blue")
varImpPlot(RandomForestModel, type=2, pch=19, col=1, cex=1.0, main="")
#########################################################################
mydata$RFPredict <- predict(RandomForestModel, newdata = mydata, type = "response")
# This is a way to combine the datasets into a single dataframe.
Pred1 <- predict(RandomForestModel, newdata = mydata, type = "prob")
Pred1DF<-data.frame(Pred1)
mydata2<- cbind(mydata, Pred1DF)
View(mydata2)
library(xlsx)
write.xlsx(mydata2, "C:/Users/dkane/Documents/RFModelExport2.xlsx", sheetName="Sheet1",
col.names=TRUE, row.names=TRUE, append=FALSE, showNA=TRUE)
mydata<-iris
library(randomForest)
#########################################################################
set.seed(126)
# The classwt function is used udring model training to assigning
# a relative cost weight for classification errors. Used to balance the errors.
RandomForestModel <- randomForest(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width, data=mydata, ntree=1, mtry=3, classwt=c(setosa=1,versicolor=3,virginica=1), importance=TRUE)
print(RandomForestModel)
library(caret)
set.seed(126)
inTrain <- createDataPartition(y = mydata$Species, p = .75,list = FALSE)
mydata<-iris
library(caret)
set.seed(85)
inTrain <- createDataPartition(y = mydata$Species, p = .75,list = FALSE)
str(inTrain)
training <- mydata[ inTrain,]
testing <- mydata[-inTrain,]
RandomForestModel <- randomForest(Species ~ Sepal.Width + Sepal.Length + Petal.Length + Petal.Width, data=training, ntree=500, mtry=3, classwt=c(setosa=1,versicolor=3,virginica=1), importance=TRUE)
rpartPred <- predict(RandomForestModel, testing, type = "class")
confusionMatrix(rpartPred, testing$Class)
confusionMatrix(rpartPred, testing$Species)
library(ISLR)
library(tree)
install.packages("ISLR")
install.packages("tree")
library(ISLR)
library(tree)
attach(carseats)
attach(Carseats)
library(ISLR)
library(tree)
attach(Carseats)
Carseats
range(sales)
range(Sales)
High = ifelse(Sales >- 8, "Yes", "No")
length(High)
Carseats
Carseats = data.frame(Carseats, High)
View(Carseats)
library(ISLR)
library(tree)
attach(Carseats)
Carseats
High = ifelse(Sales >= 8, "Yes", "No")
Carseats = data.frame(Carseats, High)
View(Carseats)
library(caret)
set.seed(2)
inTrain <- createDataPartition(y = Carseats$High, p = .75,list = FALSE)
# this then creates the actual train and test dataset
training <- mydata[ inTrain,]
testing <- mydata[-inTrain,]
training <- Carseats[ inTrain,]
testing <- Carseats[-inTrain,]
tree_model<- tree(High~., training)
plot(tree_model)
View(Carseats)
library(ISLR)
library(tree)
library(caret)
attach(Carseats)
Carseats
########################################################################
# Start the data manipulation
range(Sales) # Sales range from 0 to 16
# create a categorical variable based on Sales
High = ifelse(Sales >= 8, "Yes", "No")
length(High)
# Appends High to Carseat dataset, and now our dataset is ready.
Carseats = data.frame(Carseats, High)
set.seed(2)
inTrain <- createDataPartition(y = Carseats$Sales, p = .75,list = FALSE)
# this then creates the actual train and test dataset
training <- Carseats[ inTrain,]
testing <- Carseats[-inTrain,]
tree_model<- tree(High~., training)
plot(tree_model)
text(tree_model, pretty = 0)
Carseats = Carseats[,-1]
set.seed(2)
inTrain <- createDataPartition(y = Carseats, p = .75,list = FALSE)
# this then creates the actual train and test dataset
training <- Carseats[ inTrain,]
testing <- Carseats[-inTrain,]
View(Carseats)
set.seed(2)
inTrain <- createDataPartition(y = Carseats$High, p = .75,list = FALSE)
# this then creates the actual train and test dataset
training <- Carseats[ inTrain,]
testing <- Carseats[-inTrain,]
tree_model<- tree(High~., training)
plot(tree_model)
text(tree_model, pretty = 0)
tree_pred = predict(tree_model, testing, type="class")
confusionMatrix(tree_pred, testing$High)
mean(tree_pred != testing$High)
set.seed(3)
cv_tree = cv.tree(tree_model, FUN=prune.misclass)
names(cv_tree)
plot(cv_tree$size, cv_tree$dev, type="b")
plot(pruned_model)
text(pruned_model, pretty = 0)
pruned_model = prune.misclass(tree_model, best = 14)
plot(pruned_model)
text(pruned_model, pretty = 0)
tree_pred = predict(pruned_model, testing, type="class")
confusionMatrix(tree_pred, testing$High)
mydata<- read.csv("C:/Users/dkane/Documents/R Packages/Decision Tree/survival_unemployment.csv")
attach(mydata)
set.seed(1234)
ind <- sample(2, nrow(mydata), replace=TRUE, prob=c(0.7, 0.3))
trainData <- mydata[ind==1,]
testData <- mydata[ind==2,]
library(party)
myFormula <- event ~ ui + married + age + child + bluecoll + nonwhite
Train_Decision_Tree <- ctree(myFormula, data=trainData, controls=ctree_control(mincriterion=0.95))
table(predict(Train_Decision_Tree), trainData$event, )
table(predict(Train_Decision_Tree, trainData$event, ))
print(Train_Decision_Tree)
library(ISLR)
library(tree)
library(caret)
library(party)
attach(Carseats)
Carseats
# Start the data manipulation
range(Sales) # Sales range from 0 to 16
# create a categorical variable based on Sales
High = ifelse(Sales >= 8, "Yes", "No")
length(High)
# Appends High to Carseat dataset, and now our dataset is ready.
Carseats = data.frame(Carseats, High)
# Lets remove the "Sales" variable
Carseats = Carseats[,-1]
set.seed(2)
inTrain <- createDataPartition(y = Carseats$High, p = .75,list = FALSE)
# this then creates the actual train and test dataset
training <- Carseats[ inTrain,]
testing <- Carseats[-inTrain,]
myFormula <- High ~.
tree_model <- ctree(myFormula, data=training, controls=ctree_control(mincriterion=0.95))
# We can look at the decision tree rules.
print(tree_model)
plot(tree_model)
plot(tree_model, type="simple")
# https://www.youtube.com/watch?v=GOJN9SKl_OE
library(ISLR)
library(tree)
library(caret)
attach(Carseats)
Carseats
########################################################################
# Start the data manipulation
range(Sales) # Sales range from 0 to 16
# create a categorical variable based on Sales
High = ifelse(Sales >= 8, "Yes", "No")
length(High)
# Appends High to Carseat dataset, and now our dataset is ready.
Carseats = data.frame(Carseats, High)
# Lets remove the "Sales" variable
Carseats = Carseats[,-1]
########################################################################
### Split the data into training and testing. Code is from caret package.
set.seed(2)
inTrain <- createDataPartition(y = Carseats$High, p = .75,list = FALSE)
# this then creates the actual train and test dataset
training <- Carseats[ inTrain,]
testing <- Carseats[-inTrain,]
########################################################################
# Make a decision tree model.
tree_model<- tree(High~., training)
# show the Decision Tree
plot(tree_model)
# Add the descriptions to the tree
text(tree_model, pretty = 0)
library(ISLR)
library(tree)
library(caret)
library(party)
attach(Carseats)
Carseats
########################################################################
# Start the data manipulation
range(Sales) # Sales range from 0 to 16
# create a categorical variable based on Sales
High = ifelse(Sales >= 8, "Yes", "No")
length(High)
# Appends High to Carseat dataset, and now our dataset is ready.
Carseats = data.frame(Carseats, High)
# Lets remove the "Sales" variable
Carseats = Carseats[,-1]
########################################################################
### Split the data into training and testing. Code is from caret package.
set.seed(2)
inTrain <- createDataPartition(y = Carseats$High, p = .75,list = FALSE)
# this then creates the actual train and test dataset
training <- Carseats[ inTrain,]
testing <- Carseats[-inTrain,]
########################################################################
# Use the party package Decision Tree
myFormula <- High ~.
tree_model <- ctree(myFormula, data=training)
# We can look at the decision tree rules.
print(tree_model)
plot(tree_model)
plot(tree_model, type="simple")
require(mlbench)
data(BreastCancer)
View(BreastCancer)
BreastCancer$Id <- NULL
set.seed(2)
ind <- sample(2, nrow(BreastCancer), replace = TRUE, prob=c(0.8, 0.2))
require(mlbench)
# if you don't have any required package, use the install.packages() command
# load the data set
data(BreastCancer)
# remove the unique identifier, which is useless and would confuse the machine learning algorithms
BreastCancer$Id <- NULL
# partition the data set for 80% training and 20% evaluation (adapted from ?randomForest)
set.seed(2)
ind <- sample(2, nrow(BreastCancer), replace = TRUE, prob=c(0.8, 0.2))
# model using ctree without weights
require(party)
x.ct <- ctree(Class ~ ., data=BreastCancer[ind == 1,])
x.ct.pred <- predict(x.ct, newdata=BreastCancer[ind == 2,])
x.ct.prob <-  1- unlist(treeresponse(x.ct, BreastCancer[ind == 2,]), use.names=F)[seq(1,nrow(BreastCancer[ind == 2,])*2,2)]
# model using ctree with weights 1:10 (benign:malignant)
x.ctw <- ctree(Class ~ ., data=BreastCancer[ind == 1,], weights= ifelse(BreastCancer[ind == 1,]$Class=='benign', 1, 10))
x.ctw.pred <- predict(x.ctw, newdata=BreastCancer[ind == 2,])
x.ctw2.prob <-  1- unlist(treeresponse(x.ctw2, BreastCancer[ind == 2,]), use.names=F)[seq(1,nrow(BreastCancer[ind == 2,])*2,2)]
# Output the plot to a PNG file for display on web.  To draw to the screen,
# comment this line out.
png(filename="roc_curve_weights.png", width=700, height=700)
# plot performance
require(ROCR)
# create an ROCR prediction object from probabilities
x.ct.prob.rocr <- prediction(x.ct.prob, BreastCancer[ind == 2,'Class'])
# prepare an ROCR performance object for ROC curve (tpr=true positive rate, fpr=false positive rate)
x.ct.perf <- performance(x.ct.prob.rocr, "tpr","fpr")
# plot itplot(x.ct.perf, col=2, main="ROC curves comparing classification performance of ctree weighted vs unweighted")
# Draw a legend.
legend(0.6, 0.6, c('unweighted', 'weighted 1:10 (benign:malignant)', 'weighted 10:1'), 2:4)
# weighted 1:10
x.ctw.prob.rocr <- prediction(x.ctw.prob, BreastCancer[ind == 2,'Class'])
x.ctw.perf <- performance(x.ctw.prob.rocr, "tpr","fpr")
# add=TRUE draws on the existing chart
plot(x.ctw.perf, col=3, add=TRUE)
# weighted 10:1
x.ctw2.prob.rocr <- prediction(x.ctw2.prob, BreastCancer[ind == 2,'Class'])
x.ctw2.perf <- performance(x.ctw2.prob.rocr, "tpr","fpr")
# add=TRUE draws on the existing chart
plot(x.ctw2.perf, col=4, add=TRUE)
# close and save PNG
dev.off()
# load the mlbench package which has the BreastCancer data set
require(mlbench)
# if you don't have any required package, use the install.packages() command
# load the data set
data(BreastCancer)
# remove the unique identifier, which is useless and would confuse the machine learning algorithms
BreastCancer$Id <- NULL
# partition the data set for 80% training and 20% evaluation (adapted from ?randomForest)
set.seed(2)
ind <- sample(2, nrow(BreastCancer), replace = TRUE, prob=c(0.8, 0.2))
# model using ctree without weights
require(party)
x.ct <- ctree(Class ~ ., data=BreastCancer[ind == 1,])
x.ct.pred <- predict(x.ct, newdata=BreastCancer[ind == 2,])
x.ct.prob <-  1- unlist(treeresponse(x.ct, BreastCancer[ind == 2,]), use.names=F)[seq(1,nrow(BreastCancer[ind == 2,])*2,2)]
# model using ctree with weights 1:10 (benign:malignant)
x.ctw <- ctree(Class ~ ., data=BreastCancer[ind == 1,], weights= ifelse(BreastCancer[ind == 1,]$Class=='benign', 1, 10))
x.ctw.pred <- predict(x.ctw, newdata=BreastCancer[ind == 2,])
x.ctw2.prob <-  1- unlist(treeresponse(x.ctw2, BreastCancer[ind == 2,]), use.names=F)[seq(1,nrow(BreastCancer[ind == 2,])*2,2)]
# Output the plot to a PNG file for display on web.  To draw to the screen,
# comment this line out.
png(filename="roc_curve_weights.png", width=700, height=700)
# plot performance
require(ROCR)
# create an ROCR prediction object from probabilities
x.ct.prob.rocr <- prediction(x.ct.prob, BreastCancer[ind == 2,'Class'])
# prepare an ROCR performance object for ROC curve (tpr=true positive rate, fpr=false positive rate)
x.ct.perf <- performance(x.ct.prob.rocr, "tpr","fpr")
# plot it
plot(x.ct.perf, col=2, main="ROC curves comparing classification performance of ctree weighted vs unweighted")
# Draw a legend.
legend(0.6, 0.6, c('unweighted', 'weighted 1:10 (benign:malignant)', 'weighted 10:1'), 2:4)
# weighted 1:10
x.ctw.prob.rocr <- prediction(x.ctw.prob, BreastCancer[ind == 2,'Class'])
x.ctw.perf <- performance(x.ctw.prob.rocr, "tpr","fpr")
# add=TRUE draws on the existing chart
plot(x.ctw.perf, col=3, add=TRUE)
# weighted 10:1
x.ctw2.prob.rocr <- prediction(x.ctw2.prob, BreastCancer[ind == 2,'Class'])
x.ctw2.perf <- performance(x.ctw2.prob.rocr, "tpr","fpr")
# add=TRUE draws on the existing chart
plot(x.ctw2.perf, col=4, add=TRUE)
# close and save PNG
dev.off()
require(mlbench)
data(BreastCancer)
BreastCancer$Id <- NULL
set.seed(2)
ind <- sample(2, nrow(BreastCancer), replace = TRUE, prob=c(0.8, 0.2))
require(party)
x.ct <- ctree(Class ~ ., data=BreastCancer[ind == 1,])
x.ct.pred <- predict(x.ct, newdata=BreastCancer[ind == 2,])
x.ct.prob <-  1- unlist(treeresponse(x.ct, BreastCancer[ind == 2,]), use.names=F)[seq(1,nrow(BreastCancer[ind == 2,])*2,2)]
x.ctw <- ctree(Class ~ ., data=BreastCancer[ind == 1,], weights= ifelse(BreastCancer[ind == 1,]$Class=='benign', 1, 10))
x.ctw.pred <- predict(x.ctw, newdata=BreastCancer[ind == 2,])
x.ctw2.prob <-  1- unlist(treeresponse(x.ctw2, BreastCancer[ind == 2,]), use.names=F)[seq(1,nrow(BreastCancer[ind == 2,])*2,2)]
x.ctw <- ctree(Class ~ ., data=BreastCancer[ind == 1,], weights= ifelse(BreastCancer[ind == 1,]$Class=='benign', 1, 10))
x.ctw.pred <- predict(x.ctw, newdata=BreastCancer[ind == 2,])
x.ctw.prob <-  1- unlist(treeresponse(x.ctw, BreastCancer[ind == 2,]), use.names=F)[seq(1,nrow(BreastCancer[ind == 2,])*2,2)]
x.ctw2 <- ctree(Class ~ ., data=BreastCancer[ind == 1,], weights= ifelse(BreastCancer[ind == 1,]$Class=='benign', 10, 1))
x.ctw2.pred <- predict(x.ctw2, newdata=BreastCancer[ind == 2,])
x.ctw2.prob <-  1- unlist(treeresponse(x.ctw2, BreastCancer[ind == 2,]), use.names=F)[seq(1,nrow(BreastCancer[ind == 2,])*2,2)]
png(filename="roc_curve_weights.png", width=700, height=700)
require(ROCR)
x.ct.prob.rocr <- prediction(x.ct.prob, BreastCancer[ind == 2,'Class'])
# prepare an ROCR performance object for ROC curve (tpr=true positive rate, fpr=false positive rate)
x.ct.perf <- performance(x.ct.prob.rocr, "tpr","fpr")
# plot it
plot(x.ct.perf, col=2, main="ROC curves comparing classification performance of ctree weighted vs unweighted")
legend(0.6, 0.6, c('unweighted', 'weighted 1:10 (benign:malignant)', 'weighted 10:1'), 2:4)
x.ctw.prob.rocr <- prediction(x.ctw.prob, BreastCancer[ind == 2,'Class'])
x.ctw.perf <- performance(x.ctw.prob.rocr, "tpr","fpr")
# add=TRUE draws on the existing chart
plot(x.ctw.perf, col=3, add=TRUE)
x.ctw2.prob.rocr <- prediction(x.ctw2.prob, BreastCancer[ind == 2,'Class'])
x.ctw2.perf <- performance(x.ctw2.prob.rocr, "tpr","fpr")
# add=TRUE draws on the existing chart
plot(x.ctw2.perf, col=4, add=TRUE)
dev.off()
########################################################################
# Weighting model fit with ctree in party
# http://heuristically.wordpress.com/2010/03/15/weighting-model-fit-ctree-party/
########################################################################
# load the mlbench package which has the BreastCancer data set
require(mlbench)
# if you don't have any required package, use the install.packages() command
# load the data set
data(BreastCancer)
# remove the unique identifier, which is useless and would confuse the machine learning algorithms
BreastCancer$Id <- NULL
# partition the data set for 80% training and 20% evaluation (adapted from ?randomForest)
set.seed(2)
ind <- sample(2, nrow(BreastCancer), replace = TRUE, prob=c(0.8, 0.2))
# model using ctree without weights
require(party)
x.ct <- ctree(Class ~ ., data=BreastCancer[ind == 1,])
x.ct.pred <- predict(x.ct, newdata=BreastCancer[ind == 2,])
x.ct.prob <-  1- unlist(treeresponse(x.ct, BreastCancer[ind == 2,]), use.names=F)[seq(1,nrow(BreastCancer[ind == 2,])*2,2)]
# model using ctree with weights 1:10 (benign:malignant)
x.ctw <- ctree(Class ~ ., data=BreastCancer[ind == 1,], weights= ifelse(BreastCancer[ind == 1,]$Class=='benign', 1, 10))
x.ctw.pred <- predict(x.ctw, newdata=BreastCancer[ind == 2,])
x.ctw.prob <-  1- unlist(treeresponse(x.ctw, BreastCancer[ind == 2,]), use.names=F)[seq(1,nrow(BreastCancer[ind == 2,])*2,2)]
# model using ctree with weights 10:1 (benign:malignant)
x.ctw2 <- ctree(Class ~ ., data=BreastCancer[ind == 1,], weights= ifelse(BreastCancer[ind == 1,]$Class=='benign', 10, 1))
x.ctw2.pred <- predict(x.ctw2, newdata=BreastCancer[ind == 2,])
x.ctw2.prob <-  1- unlist(treeresponse(x.ctw2, BreastCancer[ind == 2,]), use.names=F)[seq(1,nrow(BreastCancer[ind == 2,])*2,2)]
# Output the plot to a PNG file for display on web.  To draw to the screen,
# comment this line out.
# png(filename="roc_curve_weights.png", width=700, height=700)
# plot performance
require(ROCR)
# create an ROCR prediction object from probabilities
x.ct.prob.rocr <- prediction(x.ct.prob, BreastCancer[ind == 2,'Class'])
# prepare an ROCR performance object for ROC curve (tpr=true positive rate, fpr=false positive rate)
x.ct.perf <- performance(x.ct.prob.rocr, "tpr","fpr")
# plot it
plot(x.ct.perf, col=2, main="ROC curves comparing classification performance of ctree weighted vs unweighted")
# Draw a legend.
legend(0.6, 0.6, c('unweighted', 'weighted 1:10 (benign:malignant)', 'weighted 10:1'), 2:4)
# weighted 1:10
x.ctw.prob.rocr <- prediction(x.ctw.prob, BreastCancer[ind == 2,'Class'])
x.ctw.perf <- performance(x.ctw.prob.rocr, "tpr","fpr")
# add=TRUE draws on the existing chart
plot(x.ctw.perf, col=3, add=TRUE)
# weighted 10:1
x.ctw2.prob.rocr <- prediction(x.ctw2.prob, BreastCancer[ind == 2,'Class'])
x.ctw2.perf <- performance(x.ctw2.prob.rocr, "tpr","fpr")
# add=TRUE draws on the existing chart
plot(x.ctw2.perf, col=4, add=TRUE)
setwd("C:/Users/dkane/Documents/R Packages/Titanic Tutorial")
library(reshape2)
name <- train$Name
names <- colsplit(Name, ", ", c("Last", "First"))
first <- names$First
titles <- colsplit(first, ". ", c("Title", "First"))
title <- titles$Title
fname <- titles$First
lname <- names$Last
# This combines the original table "train" with the title, last, & first names.
train <- data.frame(train, title, fname, lname)
train = read.csv("train.csv")
attach(train)
head(train)
library(reshape2)
name <- train$Name
names <- colsplit(Name, ", ", c("Last", "First"))
first <- names$First
titles <- colsplit(first, ". ", c("Title", "First"))
title <- titles$Title
fname <- titles$First
lname <- names$Last
# This combines the original table "train" with the title, last, & first names.
train <- data.frame(train, title, fname, lname)
age.model=lm(Age~ Fare + as.factor(title) + SibSp + Parch)
# this is the routine which loops through the data and imputes the difference.
for(i in 1:nrow(train)){
if(is.na(train[i, "Age"])){
train[i, "Age"] = predict(age.model, newdata=train[i,])
}
}
Train = read.csv("train data with estimated age.csv")
attach(Train)
# model with some interactions
model = glm(Survived ~ Pclass + Fare + SibSp + Parch +
Sex + Age + Pclass:Sex +
Age:Sex + SibSp:Sex, family= binomial(link="logit"))
predict(model, newdata=Train, type="response")
P = predict(model, newdata=Train, type="response")
p.survive = round(P)
library("caret")
confusionMatrix(p.survive, Survived)
test.data= read.csv("test.csv")
attach(test.data)
name <- test.data$Name
names <- colsplit(Name, ", ", c("Last", "First"))
first <- names$First
test.data1 <- colsplit(first, ". ", c("Title", "First"))
title2 <- test.data1$Title
fname <- test.data1$First
lname <- names$Last
# This combines the original table "train" with the title, last, & first names.
test.data <- data.frame(test.data, title2, fname, lname)
age.model1=lm(Age~ Fare + as.factor(title2) + SibSp + Parch)
for(i in 1:nrow(test.data)){
if(is.na(test.data[i, "Age"])){
test.data[i, "Age"] = predict(age.model1, newdata=test.data[i,])
}
}
View(test.data)
View(test.data)
View(test.data)
View(test.data)
test.data$title2[test.data$title2=="Ms"] <- Miss
test.data$title2[test.data$title2=="Ms"] <- "Miss"
View(test.data)
