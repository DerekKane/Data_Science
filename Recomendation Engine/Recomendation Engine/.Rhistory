install.packages("ridge")
install.packages("glmnet")
install.packages("lasso2")
install.packages("colorspace")
install.packages("lars")
install.packages("elasticnet")
install.packages("MASS")
library(ridge)
library(glmnet)
library(lasso2)
library(colorspace)
library(lars)
library(elasticnet)
library(MASS)
data(Prostate)
mydata <- Prostate
View(mydata)
y <- as.numeric(Prostate[,9])
x <- as.matrix(Prostate[,1:8])
select(lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,1,0.001)))
select(lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,10,0.1)))
ridge.train <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,10,0.1))
plot(seq(0,10,0.1), ridge.train$GCV, main="GCV of Ridge Regression", type="l",
xlab=expression(lambda), ylab="GCV")
lambda.ridge <- seq(0,10,0.1)[which.min(ridge.train$GCV)]
abline(v=lambda.ridge, lty=2, col="red")
colors <- rainbow_hcl(8, c = 65, l = 65) # 8 is the number or ind. variables
plot.new()
matplot(seq(0,10,0.1), coef(ridge.train)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline(v=lambda.ridge, lty=2)
text(rep(10, 9), coef(ridge.train)[length(seq(0,10,0.1)),-1], colnames(mydata)[-9], pos=4, col=colors)
threshold <- seq(0,1000,0.1)
ridge.train2 <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = threshold)
colors <- rainbow_hcl(8, c = 65, l = 65) # 8 is the number or ind. variables
plot.new()
matplot(threshold, coef(ridge.train2)[,-1], xlim=c(0,1005), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline(v=lambda.ridge, lty=2)
text(rep(10, 9), coef(ridge.train2)[length(threshold),-1], colnames(mydata)[-9], pos=4, col=colors)
ridge.reg <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = lambda.ridge)
ridge.reg
pred.ridge <- coef(ridge.reg)[1] + coef(ridge.reg)[2]*mydata[,1] +
coef(ridge.reg)[3]*mydata[,2] + coef(ridge.reg)[4]*mydata[,3] +
coef(ridge.reg)[5]*mydata[,4] + coef(ridge.reg)[6]*mydata[,5] +
coef(ridge.reg)[7]*mydata[,6] + coef(ridge.reg)[8]*mydata[,7] +
coef(ridge.reg)[9]*mydata[,8]
mydata$YHat <- coef(ridge.reg)[1] + coef(ridge.reg)[2]*mydata[,1] +
coef(ridge.reg)[3]*mydata[,2] + coef(ridge.reg)[4]*mydata[,3] +
coef(ridge.reg)[5]*mydata[,4] + coef(ridge.reg)[6]*mydata[,5] +
coef(ridge.reg)[7]*mydata[,6] + coef(ridge.reg)[8]*mydata[,7] +
coef(ridge.reg)[9]*mydata[,8]
MSE.Ridge<-(sum((mydata$lpsa-pred.ridge)^2))/nrow(mydata)
# RMSE.Ridge<-sqrt((sum((mydata$lpsa-pred.ridge)^2))/nrow(mydata))
MSE.Ridge
lassoreg.cv <- cv.glmnet(x,y, alpha=1)
plot(lassoreg.cv)
lassoreg.cv$lambda.min
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
plot(lassofits)
plot(lassofits, xvar = "lambda", label = TRUE)
plot(lassofits, xvar = "dev", label = TRUE)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
# mydata$yhat <- predict(lassof
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
plot(lassofits)
plot(lassofits, xvar = "lambda", label = TRUE)
plot(lassofits, xvar = "dev", label = TRUE)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (lassopred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
elasticreg.cv <- cv.glmnet(x,y, alpha=0.2)
plot(elasticreg.cv)
elasticreg.cv$lambda.min
elasticreg.cv$lambda.min
elasticfits <- glmnet(x,y,alpha=0.2, nlambda=100)
plot(elasticfits)
plot(elasticfits, xvar = "lambda", label = TRUE)
plot(elasticfits, xvar = "dev", label = TRUE)
elasticpred <- predict(elasticfits,x,s= elasticreg.cv$lambda.min)
elasticcoef <- predict(elasticfits,x,s=elasticreg.cv$lambda.min, type="coefficients")
elasticcoef
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (elasticpred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
ridge.linreg<- lm(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata)
summary(ridge.linreg)
ridge.linreg2<- lm(lpsa~ lcavol + lweight + svi,
data=mydata)
summary(ridge.linreg2)
linearpred <- predict(ridge.linreg2, newdata = mydata, type = "response")
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (linearpred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
library(ridge)
library(glmnet)
library(lasso2)
library(colorspace)
library(lars)
library(elasticnet)
library(MASS)
data(Prostate)
mydata <- Prostate
y <- as.numeric(Prostate[,9])
x <- as.matrix(Prostate[,1:8])
select(lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,1,0.001)))
lassoreg.cv <- cv.glmnet(x,y, alpha=1)
lassoreg.cv$lambda.min
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (lassopred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
lassofits <- glmnet(x,y,alpha=1, nlambda=1000)
plot(lassofits)
plot(lassofits, xvar = "lambda", label = TRUE)
plot(lassofits, xvar = "dev", label = TRUE)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
library(glmnet)
library(lasso2)
library(colorspace)
library(lars)
library(elasticnet)
data(Prostate)
mydata <- Prostate
y <- as.numeric(mydata[,9])
x <- as.matrix(mydata[,1:8])
model.lasso <- lars(x, y, type="lasso")
lambda.lasso <- c(model.lasso$lambda,0)
beta <- coef(model.lasso)
colors <- rainbow_hcl(8, c = 65, l = 65) # 8 is the number or ind. variables
matplot(lambda.lasso, beta, xlim=c(8,-2), type="o", pch=20, xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors)
text(rep(-0, 9), beta[9,], colnames(x), pos=4, col=colors)
abline(v=lambda.lasso[4], lty=2)
abline(h=0, lty=2)
beta.scale <- attr(model.lasso$beta, "scaled:scale")
beta.rescaled <- beta
for(j in 1:9){
beta.rescaled[j,] <- beta.rescaled[j,]*beta.scale
}
matplot(lambda.lasso, beta.rescaled, xlim=c(8,-2), type="o", pch=20, xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors)
text(rep(-0, 9), beta.rescaled[9,], colnames(x), pos=4, col=colors)
abline(v=lambda.lasso[4], lty=2)
abline(h=0, lty=2)
lassoreg.cv <- cv.glmnet(x,y, alpha=1)
plot(lassoreg.cv)
lassoreg.cv$lambda.min
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
plot(lassofits)
plot(lassofits, xvar = "lambda", label = TRUE)
plot(lassofits, xvar = "dev", label = TRUE)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
set.seed(123)
library(ridge)
library(glmnet)
library(lasso2)
library(colorspace)
library(lars)
library(elasticnet)
library(MASS)
data(Prostate)
mydata <- Prostate
y <- as.numeric(Prostate[,9])
x <- as.matrix(Prostate[,1:8])
select(lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,1,0.001)))
select(lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,10,0.1)))
ridge.train <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = seq(0,10,0.1))
plot(seq(0,10,0.1), ridge.train$GCV, main="GCV of Ridge Regression", type="l",
xlab=expression(lambda), ylab="GCV")
lambda.ridge <- seq(0,10,0.1)[which.min(ridge.train$GCV)]
abline(v=lambda.ridge, lty=2, col="red")
colors <- rainbow_hcl(8, c = 65, l = 65) # 8 is the number or ind. variables
plot.new()
matplot(seq(0,10,0.1), coef(ridge.train)[,-1], xlim=c(0,11), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline(v=lambda.ridge, lty=2)
text(rep(10, 9), coef(ridge.train)[length(seq(0,10,0.1)),-1], colnames(mydata)[-9], pos=4, col=colors)
threshold <- seq(0,1000,0.1)
ridge.train2 <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = threshold)
colors <- rainbow_hcl(8, c = 65, l = 65) # 8 is the number or ind. variables
plot.new()
matplot(threshold, coef(ridge.train2)[,-1], xlim=c(0,1005), type="l",xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors, lty=1, lwd=2, main="Ridge coefficients")
abline(v=lambda.ridge, lty=2)
text(rep(10, 9), coef(ridge.train2)[length(threshold),-1], colnames(mydata)[-9], pos=4, col=colors)
ridge.reg <- lm.ridge(lpsa~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45,
data=mydata, lambda = lambda.ridge)
# Printing the ridge-regression coefficient estimates for this problem:
ridge.reg
pred.ridge <- coef(ridge.reg)[1] + coef(ridge.reg)[2]*mydata[,1] +
coef(ridge.reg)[3]*mydata[,2] + coef(ridge.reg)[4]*mydata[,3] +
coef(ridge.reg)[5]*mydata[,4] + coef(ridge.reg)[6]*mydata[,5] +
coef(ridge.reg)[7]*mydata[,6] + coef(ridge.reg)[8]*mydata[,7] +
coef(ridge.reg)[9]*mydata[,8]
mydata$YHat <- coef(ridge.reg)[1] + coef(ridge.reg)[2]*mydata[,1] +
coef(ridge.reg)[3]*mydata[,2] + coef(ridge.reg)[4]*mydata[,3] +
coef(ridge.reg)[5]*mydata[,4] + coef(ridge.reg)[6]*mydata[,5] +
coef(ridge.reg)[7]*mydata[,6] + coef(ridge.reg)[8]*mydata[,7] +
coef(ridge.reg)[9]*mydata[,8]
MSE.Ridge<-(sum((mydata$lpsa-pred.ridge)^2))/nrow(mydata)
# RMSE.Ridge<-sqrt((sum((mydata$lpsa-pred.ridge)^2))/nrow(mydata))
MSE.Ridge
lassoreg.cv <- cv.glmnet(x,y, alpha=1)
plot(lassoreg.cv)
lassoreg.cv$lambda.min
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
plot(lassofits)
plot(lassofits, xvar = "lambda", label = TRUE)
plot(lassofits, xvar = "dev", label = TRUE)
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (lassopred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
library(neuralnet)
View(mydata)
View(mydata)
mydata$YHat <- NULL
set.seed(1234567890)
Churnnet <- neuralnet(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, mydata, hidden = 4, lifesign = "minimal",
linear.output = FALSE, learningrate = 0.3, threshold = 0.10, algorithm = "backprop", rep=2)
library(neuralnet)
install.packages(neuralnet)
install.packages("neuralnet")
library(neuralnet)
set.seed(1234567890)
Churnnet <- neuralnet(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, mydata, hidden = 4, lifesign = "minimal",
linear.output = FALSE, learningrate = 0.3, threshold = 0.10, algorithm = "backprop", rep=2)
Churnnet <- neuralnet(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, mydata, hidden = 4, lifesign = "minimal",
linear.output = FALSE, learningrate = 0.3, threshold = 0.10, algorithm = "backprop", rep=100)
plot(Churnnet, rep = "best")
NNResult <- compute(Churnnet, mydata[,1:8])
mydata$YHat4 <- NNResult$net.result[1]
View(mydata)
mydata$YHat4 <- NULL
set.seed(1234567890)
Churnnet <- neuralnet(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, mydata, hidden = 4, lifesign = "minimal",
linear.output = FALSE, learningrate = 0.3, threshold = 0.10, rep=100)
NNResult <- compute(Churnnet, mydata[,1:8])
mydata$YHat4 <- NNResult$net.result[1]
mydata
install.packages("caret")
library(caret)
NNet <- avNNet(lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45, data = mydata, repeats = 25, size=5, decay=0.3, linout=TRUE)
plot(NNet, rep = "best")
mydata$YHat4 <- predict(NNet, mydata)
mydata
setwd("C:/Users/Derek/Documents/RPackages/Business Metrics/")
setwd("C:/Users/dkane/Documents/R Scripts/Recomendation Engine/")
data <- read.csv("beerreview.csv")
summary(data)
data <- read.csv("beerreview.csv")
summary(data)
common_reviewers_by_id <- function(beer1, beer2) {
reviews1 <- subset(data, beer_id==beer1)
reviews2 <- subset(data, beer_id==beer2)
reviewers_sameset <- intersect(reviews1[,'review_profilename'],
reviews2[,'review_profilename'])
if (length(reviewers_sameset)==0) {
NA
} else {
reviewers_sameset
}
}
beer_lookup <- data[,c("beer_id", "beer_name")]
View(beer_lookup)
beer_lookup <- beer_lookup[duplicated(beer_lookup)==FALSE,]
beer_lookup <- data[,c("beer_id", "beer_name")]
beer_lookup <- beer_lookup[duplicated(beer_lookup)==FALSE,]
setwd("C:/Users/dkane/Documents/R Scripts/Recomendation Engine/")
data <- read.csv("beerreview.csv")
summary(data)
na.omit(data)
data <- na.omit(data)
common_reviewers_by_id <- function(beer1, beer2) {
reviews1 <- subset(data, beer_id==beer1)
reviews2 <- subset(data, beer_id==beer2)
reviewers_sameset <- intersect(reviews1[,'review_profilename'],
reviews2[,'review_profilename'])
if (length(reviewers_sameset)==0) {
NA
} else {
reviewers_sameset
}
}
beer_lookup <- data[,c("beer_id", "beer_name")]
beer_lookup <- beer_lookup[duplicated(beer_lookup)==FALSE,]
common_reviewers_by_name <- function(name1, name2) {
beer1 <- subset(beer_lookup, beer_name==name1)$beer_id
beer2 <- subset(beer_lookup, beer_name==name2)$beer_id
common_reviewers_by_id(beer1, beer2)
}
common_reviewers_by_id(5, 17)
common_reviewers_by_name("Appalachian 666", "Alameda Black Bear XX Stout")
features <- c("review_overall", "review_aroma", "review_palate", "review_taste")
get_review_metrics <- function(beer, userset) {
beer.data <- subset(data, beer_id==beer & review_profilename %in% userset)
o <- order(beer.data$review_profilename)
beer.data <- beer.data[o,]
dups <- duplicated(beer.data$review_profilename)==FALSE
beer.data <- beer.data[dups,]
#this can return more than 1 type of metric
beer.data[,features]
}
head(reviews)
head(beer.data)
get_review_metrics <- function(beer, userset) {
beer.data <- subset(data, beer_id==beer & review_profilename %in% userset)
o <- order(beer.data$review_profilename)
beer.data <- beer.data[o,]
dups <- duplicated(beer.data$review_profilename)==FALSE
beer.data <- beer.data[dups,]
#this can return more than 1 type of metric
beer.data[,features]
}
head(reviews)
head(get_review_metrics)
head(reviews)
calc_similarity <- function(b1, b2) {
common_users <- common_reviewers_by_id(b1, b2)
if (is.na(common_users)) {
return (NA)
}
beer1.reviews <- get_reviews(b1, common_users)
beer2.reviews <- get_reviews(b2, common_users)
#this can be more complex; we're just taking a weighted average
weights <- c(2, 1, 1, 1)
corrs <- sapply(names(beer1.reviews), function(metric) {
cor(beer1.reviews[metric], beer2.reviews[metric])
})
sum(corrs * weights, na.rm=TRUE)
}
b1 <- beer_name_to_id("Dicks Imperial IPA")
b2 <- beer_name_to_id("Highland Gaelic Ale")
calc_similarity(b1, b2)
install.packages("tm")
install.packages("Snowball")
library(tm)
install.packages("tm")
install.packages("C:/Users/dkane/Downloads/tm_0.5-9.tar.gz", repos = NULL, type = "source")
install.packages("C:/Users/dkane/Downloads/tm_0.5-9.tar.gz", repos = NULL, type = "source")
