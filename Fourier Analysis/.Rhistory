getSymbols("FB", src = "yahoo", from = "1900-01-01", to = "2014-04-01")
chartSeries(FB, theme="black")
FB_Subset <- window(FB, start=as.Date("2013-01-01"), end = as.Date("2014-04-01"))
FB_Train <- cbind(FB_Subset$FB.Close - FB_Subset$FB.Open)
library(RHmm)
hm_model <- HMMFit(obs=FB_Train, dis="MIXTURE", nStates=5, nMixt=4)
print(hm_model)
hm_model <- HMMFit(obs=FB_Train, dis="MIXTURE", nStates=5, nMixt=4, control=list(iter=2000))
TWII_Subset$FB.Open
print(hm_model)
VitPath <- viterbi(hm_model, FB_Train)
FB_Predict <- cbind(FB_Subset$FB.Close, VitPath$states)
chartSeries(FB_Predict[,1])
addTA(FB_Predict[FB_Predict[,2]==1,1],on=1,type="p",col=5,pch=25)
addTA(FB_Predict[FB_Predict[,2]==2,1],on=1,type="p",col=6,pch=24)
addTA(FB_Predict[FB_Predict[,2]==3,1],on=1,type="p",col=7,pch=23)
addTA(FB_Predict[FB_Predict[,2]==4,1],on=1,type="p",col=8,pch=22)
addTA(FB_Predict[FB_Predict[,2]==5,1],on=1,type="p",col=10,pch=21)
change <- sum(hm_model$HMM$transMat[last(VitPath$states),] * .colSums((matrix(unlist(hm_model$HMM$distribution$mean),nrow=4,ncol=5)) * (matrix(unlist(hm_model$HMM$distribution$proportion), nrow=4,ncol=5)), m=4,n=5))
mydata <- last(FB_Subset)
View(mydata)
predict <- mydata$FB.Open + change
View(predict)
View(mydata)
predict$FB.Open <- mydata$FB.Open + change
View(predict)
predict$FB.Open
require(devEMF)
require(devEMF)
install.packages('devEMF')
require(devEMF)
library(quantmod)
library(RHmm)
library(parallel)
getSymbols("FB", src = "yahoo", from = "1900-01-01", to = "2014-04-01")
print(head(FB))
print(tail(FB))
chartSeries(FB, theme="white")
trainsetraw <- window(FB, start = as.Date("1900-01-01"), end = as.Date("2013-04-01"))
print(ncol(trainsetraw)-1)
print(trainsetraw[,1:ncol(trainsetraw)-1])
trainset <- na.omit(trainsetraw[,1:ncol(trainsetraw)-1])
View(trainset)
print(trainset)
train <- cbind(trainset$FB.Close - trainset$FB.Open)
testset <- window(FB, start = as.Date("2013-04-02"), end = as.Date("2014-04-01"))
test <- cbind(testset$FB.Close - testset$FB.Open)
print(testset)
hm_model <- HMMFit(obs = train, nStates = 5, nMixt = 4, dis = "MIXTURE")
print(hm_model)
hm_model <- HMMFit(obs = train, nStates = 5, nMixt = 4, dis = "MIXTURE", control=list(iter=2000))
print(hm_model)
hm_model <- HMMFit(obs = train, nStates = 5, nMixt = 4, dis = "MIXTURE", control=list(iter=2000))
print(hm_model)
require(devEMF)
library(quantmod)
library(RHmm)
library(parallel)
#png('FB.png')
getSymbols("FB", src = "yahoo", from = "1900-01-01", to = "2014-04-01")
#getSymbols("FB", src = "google")
#getSymbols("FB")
print(head(FB))
print(tail(FB))
chartSeries(FB, theme="white")
#trainset <- window(FB, start = as.Date("1900-01-01"), end = as.Date("2013-04-01"))
trainsetraw <- window(FB, start = as.Date("1900-01-01"), end = as.Date("2013-04-01"))
print(ncol(trainsetraw)-1)
print(trainsetraw[,1:ncol(trainsetraw)-1])
trainset <- na.omit(trainsetraw[,1:ncol(trainsetraw)-1])
print(trainset)
#FB_Subset <- window(FB, start = as.Date("1900-01-01"), end = as.Date("2013-04-01"))
#FB_Train <- cbind(FB_Subset$FB.Close - FB_Subset$FB.Open, FB_Subset$FB.Volume)
train <- cbind(trainset$FB.Close - trainset$FB.Open)
#print(train)
testset <- window(FB, start = as.Date("2013-04-02"), end = as.Date("2014-04-01"))
test <- cbind(testset$FB.Close - testset$FB.Open)
print(testset)
# Baum-Welch Algorithm to find the model for the given observations
#hm_model <- HMMFit(obs = FB_Train, nStates = 5)
hm_model <- HMMFit(obs = train, nStates = 5, nMixt = 4, dis = "MIXTURE", control=list(iter=2000))
print(hm_model)
VitPath <- viterbi (hm_model, train)
print(VitPath)
png('FB.png')
FB_Predict <- cbind(trainset$FB.Close, VitPath$states)
testset <- cbind(testset, Pred = 0)
rows = nrow(testset)
rows
MAPEsum = 0
NRMSEsum = 0
for (i in 1: rows) {
#if (i == rows) break
if(i != 0) {
testrow <- testset[i, ]
#print(testrow)
todayopen <- testset$FB.Open[i, ]
actual <- testset$FB.Close[i, ]
#todayclose <- testset$FB.Close[i, ]
}
change <- sum(hm_model$HMM$transMat[last(VitPath$states),] * .colSums((matrix(unlist(hm_model$HMM$distribution$mean), nrow=4,ncol=5)) * (matrix(unlist(hm_model$HMM$distribution$proportion), nrow=4,ncol=5)), m=4,n=5))
print(change)
pred <- todayopen + change
testset[i, ]$Pred <- pred
print(testset[i, ])
diff = (abs ((pred - actual)/ actual))[1,]$FB.Open
MAPEsum <- sum(MAPEsum, diff[1,1])
NRMSEsum <- sum(NRMSEsum, (pred - actual)^2)
train <- rbind (train, todayclose - todayopen)
hm_model <- HMMFit(obs = train, nStates = 5, nMixt = 4, dis = "MIXTURE")
VitPath <- viterbi (hm_model, train)
}
print(paste0("[Stat] Rows = ", rows))
MAPE <- MAPEsum*100/rows
print(paste0("[Stat] MAPEsum = ", MAPEsum))
print(paste0("[Stat] MAPE = ", MAPE))
actuals <- testset$FB.Close
ymax = max (actuals)
ymin = min (actuals)
source('~/.active-rstudio-document', echo=TRUE)
NRMSE <- sqrt(NRMSEsum)/(rows * (ymax - ymin))
print(paste0("[Stat] NRMSE = ", NRMSE))
chartSeries(testset[1:rows, 1], theme= chartTheme('white', up.col = 'blue'), name = "FB", legend = "Actual",
TA = "addTA(testset[1:rows, ncol(testset)], on = 1, col='red')")
postscript('FB.eps')
chartSeries(testset[1:rows, 1], theme= chartTheme('white', up.col = 'blue'), name = "FB", legend = "Actual",
TA = "addTA(testset[1:rows, ncol(testset)], on = 1, col='red')")
library(quantmod)
getSymbols("FB", src = "yahoo", from = "1900-01-01", to = "2014-04-01")
chartSeries(FB, theme="black")
View(FB)
library(quantmod)
getSymbols("FB", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
chartSeries(FB, theme="black")
library(quantmod)
getSymbols("FB", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
chartSeries(FB, theme="black")
FB_Subset <- window(FB, start=as.Date("2013-01-01"), end = as.Date("2015-12-31"))
FB_Train <- cbind(FB_Subset$FB.Close - FB_Subset$FB.Open)
library(RHmm)
hm_model <- HMMFit(obs=FB_Train, dis="MIXTURE", nStates=5, nMixt=4, control=list(iter=2000))
print(hm_model)
VitPath <- viterbi(hm_model, FB_Train)
FB_Predict <- cbind(FB_Subset$FB.Close, VitPath$states)
chartSeries(FB_Predict[,1])
addTA(FB_Predict[FB_Predict[,2]==1,1],on=1,type="p",col=5,pch=25)
addTA(FB_Predict[FB_Predict[,2]==2,1],on=1,type="p",col=6,pch=24)
addTA(FB_Predict[FB_Predict[,2]==3,1],on=1,type="p",col=7,pch=23)
addTA(FB_Predict[FB_Predict[,2]==4,1],on=1,type="p",col=8,pch=22)
addTA(FB_Predict[FB_Predict[,2]==5,1],on=1,type="p",col=10,pch=21)
change <- sum(hm_model$HMM$transMat[last(VitPath$states),] * .colSums((matrix(unlist(hm_model$HMM$distribution$mean),nrow=4,ncol=5)) * (matrix(unlist(hm_model$HMM$distribution$proportion), nrow=4,ncol=5)), m=4,n=5))
mydata <- last(FB_Subset)
predict <- mydata$FB.Open + change
predict$FB.Open
getSymbols("GEA", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
getSymbols("apple", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
getSymbols("GOOG", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
chartSeries(GOOG, theme="black")
GOOG_Subset <- window(GOOG, start=as.Date("2013-01-01"), end = as.Date("2015-12-31"))
GOOG_Train <- cbind(GOOG_Subset$GOOG.Close - GOOG_Subset$GOOG.Open)
library(RHmm)
hm_model <- HMMFit(obs=GOOG_Train, dis="MIXTURE", nStates=5, nMixt=4, control=list(iter=2000))
print(hm_model)
VitPath <- viterbi(hm_model, GOOG_Train)
GOOG_Predict <- cbind(GOOG_Subset$GOOG.Close, VitPath$states)
chartSeries(GOOG_Predict[,1])
addTA(GOOG_Predict[GOOG_Predict[,2]==1,1],on=1,type="p",col=5,pch=25)
addTA(GOOG_Predict[GOOG_Predict[,2]==2,1],on=1,type="p",col=6,pch=24)
addTA(GOOG_Predict[GOOG_Predict[,2]==3,1],on=1,type="p",col=7,pch=23)
addTA(GOOG_Predict[GOOG_Predict[,2]==4,1],on=1,type="p",col=8,pch=22)
addTA(GOOG_Predict[GOOG_Predict[,2]==5,1],on=1,type="p",col=10,pch=21)
getSymbols("TWII", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
getSymbols("^TWII", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
chartSeries(TWII, theme="black")
TWII_Subset <- window(TWII, start=as.Date("2013-01-01"), end = as.Date("2015-12-31"))
TWII_Train <- cbind(TWII_Subset$TWII.Close - TWII_Subset$TWII.Open)
library(RHmm)
hm_model <- HMMFit(obs=TWII_Train, dis="MIXTURE", nStates=5, nMixt=4, control=list(iter=2000))
print(hm_model)
VitPath <- viterbi(hm_model, TWII_Train)
TWII_Predict <- cbind(TWII_Subset$TWII.Close, VitPath$states)
chartSeries(TWII_Predict[,1])
addTA(TWII_Predict[TWII_Predict[,2]==1,1],on=1,type="p",col=5,pch=25)
addTA(TWII_Predict[TWII_Predict[,2]==2,1],on=1,type="p",col=6,pch=24)
addTA(TWII_Predict[TWII_Predict[,2]==3,1],on=1,type="p",col=7,pch=23)
addTA(TWII_Predict[TWII_Predict[,2]==4,1],on=1,type="p",col=8,pch=22)
addTA(TWII_Predict[TWII_Predict[,2]==5,1],on=1,type="p",col=10,pch=21)
change <- sum(hm_model$HMM$transMat[last(VitPath$states),] * .colSums((matrix(unlist(hm_model$HMM$distribution$mean),nrow=4,ncol=5)) * (matrix(unlist(hm_model$HMM$distribution$proportion), nrow=4,ncol=5)), m=4,n=5))
mydata <- last(TWII_Subset)
predict <- mydata$TWII.Open + change
predict$TWII.Open
View(mydata)
library(quantmod)
getSymbols("^TWII", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
chartSeries(TWII, theme="black")
View(TWII)
write.csv(TWII, file='C:/Users/Derek/Documents/mydata.csv', row.names=F)
TWII_Subset <- window(TWII, start=as.Date("2013-01-01"), end = as.Date("2015-12-31"))
TWII_Train <- cbind(TWII_Subset$TWII.Close - TWII_Subset$TWII.Open)
library(RHmm)
# hm_model <- HMMFit(obs=TWII_Train, nStates=5)
hm_model <- HMMFit(obs=TWII_Train, nStates=5)
print(hm_model)
library(quantmod)
getSymbols("^TWII", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
chartSeries(TWII, theme="black")
TWII_Subset <- window(TWII, start=as.Date("2013-01-01"), end = as.Date("2015-12-31"))
TWII_Train <- cbind(TWII_Subset$TWII.Close - TWII_Subset$TWII.Open)
library(RHmm)
# Baum-Welch Algorithm
# hm_model <- HMMFit(obs=TWII_Train, nStates=5)
hm_model <- HMMFit(obs=TWII_Train, dis="MIXTURE", nStates=5, nMixt=4, control=list(iter=2000))
print(hm_model)
library(quantmod)
getSymbols("^TWII", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
chartSeries(TWII, theme="black")
TWII_Subset <- window(TWII, start=as.Date("2013-01-01"), end = as.Date("2015-12-31"))
TWII_Train <- cbind(TWII_Subset$TWII.Close - TWII_Subset$TWII.Open)
library(RHmm)
hm_model <- HMMFit(obs=TWII_Train, nStates=5)
print(hm_model)
change <- sum(hm_model$HMM$transMat[last(VitPath$states),] * .colSums((matrix(unlist(hm_model$HMM$distribution$mean),nrow=2,ncol=5)) * (matrix(unlist(hm_model$HMM$distribution$proportion), nrow=2,ncol=5)), m=2,n=5))
VitPath <- viterbi(hm_model, TWII_Train)
change <- sum(hm_model$HMM$transMat[last(VitPath$states),] * .colSums((matrix(unlist(hm_model$HMM$distribution$mean),nrow=2,ncol=5)) * (matrix(unlist(hm_model$HMM$distribution$proportion), nrow=2,ncol=5)), m=2,n=5))
#################################################################################
# Hidden Markov Model Example
#################################################################################
# http://www.slideshare.net/ChiuYW/hidden-markov-model-stock-prediction
# https://github.com/david78k/stock/blob/master/rhmm/TWII.R
# http://stackoverflow.com/questions/11644522/getting-the-next-observation-from-a-hmm-gaussian-mixture-distribution
# install.packages("quantmod")
library(quantmod)
getSymbols("^TWII", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
chartSeries(TWII, theme="black")
TWII_Subset <- window(TWII, start=as.Date("2013-01-01"), end = as.Date("2015-12-31"))
TWII_Train <- cbind(TWII_Subset$TWII.Close - TWII_Subset$TWII.Open)
# TWII_Train <- cbind(TWII_Subset$TWII.Close - TWII_Subset$TWII.Open,
#                  TWII_Subset$TWII.Volume)
#################################################################################
# Baum-Welch Algorithm
#################################################################################
library(RHmm)
# Package Located at https://r-forge.r-project.org/R/?group_id=85
# hm_model <- HMMFit(obs=TWII_Train, nStates=5)
hm_model <- HMMFit(obs=TWII_Train, dis="MIXTURE", nStates=5, nMixt=4,
control=list(iter=2000))
print(hm_model)
#################################################################################
# Hidden Markov Model Example
#################################################################################
# http://www.slideshare.net/ChiuYW/hidden-markov-model-stock-prediction
# https://github.com/david78k/stock/blob/master/rhmm/TWII.R
# http://stackoverflow.com/questions/11644522/getting-the-next-observation-from-a-hmm-gaussian-mixture-distribution
# install.packages("quantmod")
library(quantmod)
getSymbols("^TWII", src = "yahoo", from = "1900-01-01", to = "2015-12-31")
chartSeries(TWII, theme="black")
TWII_Subset <- window(TWII, start=as.Date("2013-01-01"), end = as.Date("2015-12-31"))
TWII_Train <- cbind(TWII_Subset$TWII.Close - TWII_Subset$TWII.Open)
# TWII_Train <- cbind(TWII_Subset$TWII.Close - TWII_Subset$TWII.Open,
#                  TWII_Subset$TWII.Volume)
#################################################################################
# Baum-Welch Algorithm
#################################################################################
library(RHmm)
# Package Located at https://r-forge.r-project.org/R/?group_id=85
# hm_model <- HMMFit(obs=TWII_Train, nStates=5)
hm_model <- HMMFit(obs=TWII_Train, dis="MIXTURE", nStates=5, nMixt=4,control=list(iter=2000))
print(hm_model)
View(TWII_Train)
VitPath <- viterbi(hm_model, TWII_Train)
VitPath
hm_model
VitPath <- viterbi(hm_model, TWII_Train)
#################################################################################
# Predict the known values of the HMM model
#################################################################################
TWII_Predict <- cbind(TWII_Subset$TWII.Close, VitPath$states)
View(TWII_Predict)
View(TWII_Predict)
chartSeries(TWII_Predict[,1])
addTA(TWII_Predict[TWII_Predict[,2]==1,1],on=1,type="p",col=5,pch=25)
addTA(TWII_Predict[TWII_Predict[,2]==2,1],on=1,type="p",col=6,pch=24)
addTA(TWII_Predict[TWII_Predict[,2]==3,1],on=1,type="p",col=7,pch=23)
addTA(TWII_Predict[TWII_Predict[,2]==4,1],on=1,type="p",col=8,pch=22)
addTA(TWII_Predict[TWII_Predict[,2]==5,1],on=1,type="p",col=10,pch=21)
change <- sum(hm_model$HMM$transMat[last(VitPath$states),] * .colSums((matrix(unlist(hm_model$HMM$distribution$mean),nrow=4,ncol=5)) * (matrix(unlist(hm_model$HMM$distribution$proportion), nrow=4,ncol=5)), m=4,n=5))
mydata <- last(TWII_Subset)
predict <- mydata$TWII.Open + change
predict$TWII.Open
mydata
library(genalg)
library(ggplot2)
library(animation)
# Create the table for the knapsack problem.
dataset <- data.frame(item = c("pocketknife", "beans", "potatoes",
"unions", "sleeping bag", "rope", "compass"),
survivalpoints = c(10, 20, 15, 2, 30, 10, 30),
weight = c(1, 5, 10, 1, 7, 5, 1))
# Create a variable to represent the maximum weightlimit.
weightlimit <- 20
View(dataset)
chromosome = c(1, 0, 0, 1, 1, 0, 0)
dataset[chromosome == 1, ]
# We can check to what amount of surivival points this configuration sums up.
cat(chromosome %*% dataset$survivalpoints)
evalFunc <- function(x) {
current_solution_survivalpoints <- x %*% dataset$survivalpoints
current_solution_weight <- x %*% dataset$weight
if (current_solution_weight > weightlimit)
return(0) else return(-current_solution_survivalpoints)
}
iter = 100
GAmodel <- rbga.bin(size = 7, popSize = 200, iters = 100, mutationChance = 0.01,
elitism = T, evalFunc = evalFunc)
cat(summary.rbga(GAmodel))
solution = c(1, 1, 0, 1, 1, 1, 1)
dataset[solution == 1, ]
library("GA")
data("fat", package="UsingR")
mydata <- fat
View(mydata)
model <- lm(body.fat.siri ~ age + weight + height + neck + chest + abdomen
+ hip + thigh + knee + ankle + bicep + forearm + wrist, data=mydata)
summary(model)
library("GA")
# The design matrix without the intercept is extracted from the
# fitted model object by:
x <- model.matrix(model)[,-1]
y <- model.response(model.frame(model))
# Then the fitness function can be maximized by the following:
fitness <- function(string){
inc <- which(string == 1)
X <- cbind(1, x[,inc])
mod <- lm.fit(X,y)
class(mod) <- "lm"
-AIC(mod)
}
GA <- ga("binary", fitness = fitness, nBits = ncol(x), names= colnames(x), monitor=plot)
plot(GA)
summary(GA)
model2 <- lm(body.fat.siri~.,
data=data.frame(body.fat.siri = y, x[,GA@solution == 1]))
summary(model2)
library(GA)
data(airquality)
airquality <- na.omit(airquality)
View(airquality)
OLS <- function(data, b0, b1, b2){
attach(data, warn.conflicts=F)
Y_hat <- b0  + b1*Wind + b2*Temp
SSE = t(Ozone-Y_hat) %*% (Ozone-Y_hat) #matrix formulation for SSE
detach(data)
return(SSE)
}
ga.OLS <- ga(type='real-valued', min=c(-100,-100, -100),
max=c(100, 100, 100), popSize=500, maxiter=500, names=c('intercept', 'Wind', 'Temp'),
keepBest=T, fitness = function(b) -OLS(airquality, b[1],b[2], b[3]))
ga.model <- summary(ga.OLS)
ga.model
plot(ga.model)
lm.model <- lm(formula= Ozone ~ Wind + Temp, data=airquality)
summary(lm.model)
lm.model$res %*% lm.model$res ### SSE.lm
-ga.model$fitness ### SSE.ga
lm.model$res %*% lm.model$res + ga.model$fitness  ### difference between OLS and GA's SSE
Total.Lower <- 62300
Total.Upper <- 85200
std.dev <- sd(Total.Lower:Total.Upper)
average <- mean(Total.Lower:Total.Upper)
std.dev2 <- sd(average)
std.dev2 <- sd(Total.Lower,Total.Upper,average)
std.dev2 <- sd(Total.Lower:Total.Upper:average)
std.dev2 <- sd(std.dev:average)
std.p <- sqrt(sum((foo - mean(foo))^2) / length(foo))
foo <- c(Total.Lower,Total.Upper)
std.p <- sqrt(sum((foo - mean(foo))^2) / length(foo))
std.dev2 <- sd(std.p:average)
foo <- c(Total.Lower,Total.Upper, average)
std.p <- sqrt(sum((foo - mean(foo))^2) / length(foo))
popSD <- function(x) {
return(sqrt(sum((x - mean(x))^2) / length(x)))
}
std.p <- popSD(foo)
std.p
randvar <- average / 50
min.iterations <- ((3 *std.p)/randvar)^2
min.iterations <- ceiling(((3 *std.p)/randvar)^2)
Lower.ASP <- 0.315 # Lower Bound for ASP
Upper.ASP <- 0.365 # Upper Bound for ASP
Lower.Qty <- 350000 # Lower Bound for the Qty
Upper.Qty <- 450000 # Upper Bound for the Qty
runs <- min.iterations
ASP.sample <- rbeta(runs, Lower.ASP, Upper.ASP)
ASP.sample <- runif(runs, min=Lower.ASP, max=Upper.ASP)
ASP.sample
Qty.sample <- runif(runs, min=Lower.Qty, max=Upper.Qty)
Revenue <- ASP.sample*Qty.sample
Revenue
MC.Estimation <- mean(Revenue)
MC.Estimation
MC.Upper95 <- MC.Estimation + popSD(Revenue)
MC.Lower95 <- MC.Estimation - popSD(Revenue)
runs <- 100000
ASP.sample <- runif(runs, min=Lower.ASP, max=Upper.ASP)
Qty.sample <- runif(runs, min=Lower.Qty, max=Upper.Qty)
Revenue <- ASP.sample*Qty.sample
MC.Estimation <- mean(Revenue)
MC.Upper95 <- MC.Estimation + popSD(Revenue)
MC.Lower95 <- MC.Estimation - popSD(Revenue)
setwd("C:/Users/Derek/Documents/RPackages/Fourier Analysis")
library('ggplot2')
library('nnet')
library('gridBase')
orderts <- read.csv("C:/Users/Derek/Documents/RPackages/Fourier Analysis/orderts.csv")
qplot(week, orders, data = orderts, colour = as.factor(year), geom = "line")
orderts2 <- cbind(orderts[-13,], weekinq=c(1:117))
prev <- orderts2[1,]
runvar <- 1
for(i in 2:nrow(orderts2)){
current <- orderts2[i,]
orderts2[i,"weekinq"] <- ifelse(prev$quarter == current$quarter, runvar+1, 1)
runvar <- ifelse(prev$quarter == current$quarter, runvar+1, 1)
prev <- current
}
rm(prev, current, runvar, i)
qplot(weekinq, orders, data = orderts2, colour = as.factor(year), geom = "line") +
facet_wrap(~quarter)
f <- data.frame(coef = fft(orderts2[1:104, "orders"]), freqindex = c(1:104))
qplot(freqindex, Mod(coef), data = f[2:53,], geom = "line")
f[Mod(f$coef) > 3 & f$freqindex < 53, "freqindex"] - 1
peaks <- Mod(f$coef) > 3
ffilt <- f
ffilt[!peaks, "coef"] <- 0
ffilt <- data.frame(index=ffilt$freqindex, value=Re(fft(ffilt$coef, inverse=TRUE))/104, type=rep("filtered", times=104))
ffilt <- rbind(ffilt, data.frame(index=seq(1:104), value=orderts2[1:104,"orders"], type=rep("original", times=104)))
midindex <- ceiling((length(f$coef)-1)/ 2) + 1
lindex <- length(f$coef)
peakind <- f[abs(f$coef) > 3 & f$freqindex > 1 & f$freqindex < midindex,]
lowerind <- 1
subsignals <- lapply(c(peakind$freqindex, midindex+1), function(x){
upperind <- x
fsub <- f
notnullind <- ((fsub$freqindex >= lowerind
& fsub$freqindex < upperind)
|
(fsub$freqindex >  (lindex - upperind + 2)
& fsub$freqindex <= (lindex - lowerind + 2)))
fsub[!notnullind,"coef"] <- 0
lowerind <<- upperind
Re(fft(fsub$coef, inverse=TRUE)/length(fsub$coef))
})
grid.newpage()
pushViewport(viewport(layout=grid.layout(4,2)))
vplayout <- function(x,y)
viewport(layout.pos.row = x, layout.pos.col = y)
psig <- function(x, y, z){
h <- data.frame(index = c(1:length(subsignals[[x]])),
orders = subsignals[[x]])
lab <- paste("Subseries ", as.character(x), sep="")
print(qplot(index, orders, data = h, geom = "line", main=lab), vp = vplayout(y,z))
TRUE
}
psig(1,1,1); psig(2,1,2); psig(3,2,1); psig(4,2,2); psig(5,3,1); psig(6,3,2); psig(7,4,1)
nn.sizes <- c(4,2,3,3,3,2,2,2)
numofsubs <- length(subsignals)
twindow <- 4
offsettedsubdfs <- lapply(1:numofsubs, function(x){
singleoffsets <- lapply(0:(twindow-1), function(y){
subsignals[[x]][(twindow-y):(length(subsignals[[x]])-y-1)]
})
a <- Reduce(cbind, singleoffsets)
names <- lapply(1:twindow, function(y){paste("TS", as.character(x), "_", as.character(y), sep = "")})
b <- as.data.frame(a)
colnames(b) <- names
b
})
sample.number <- length(offsettedsubdfs[[1]][,1])
nns <- lapply(1:length(offsettedsubdfs), function(i)
{
nn <- nnet(offsettedsubdfs[[i]][1:(sample.number),], #the training samples
subsignals[[i]][(twindow+1):(length(subsignals[[i]]))], #the output
#corresponding to the training samples
size=nn.sizes[i], #number of neurons
maxit = 1000, #number of maximum iteration
linout = TRUE) #the neuron in the output layer should be linear
#the result of the trained networks should be plotted
plot(subsignals[[i]][(twindow+1):(length(subsignals[[i]]))], type="l")
lines(nn$fitted.values,type="l",col="red")
nn
})
number.of.predict <- 14
long.predictions <- lapply(1:length(offsettedsubdfs), function(i)
{
prediction <- vector(length=number.of.predict, mode="numeric")
#initial input
input <- offsettedsubdfs[[i]][sample.number,]
for (j in 1 : number.of.predict)
{
prediction[j] <- predict(nns[[i]], input)
input <- c(prediction[j],input[1:(length(input)-1)])
}
#we want to plot the prediction
plot(c(nns[[i]]$fitted.values,prediction), type="l",col="red")
lines(subsignals[[i]][(twindow+1):length(subsignals[[i]])])
prediction
})
mydata <- as.data.frame(long.predictions)
View(mydata)
write.csv(mydata, file='C:/Users/Derek/Documents/RPackages/Fourier Analysis/FFTResults.csv', row.names=F)
install.packages("grid")
install.packages("grid")
