###############################################################################
model.lasso <- lars(x, y, type="lasso")
lambda.lasso <- c(model.lasso$lambda,0)
beta <- coef(model.lasso)
# set the colors for the output.
colors <- rainbow_hcl(8, c = 65, l = 65) # 8 is the number or ind. variables
# Plot the lasso
matplot(lambda.lasso, beta, xlim=c(8,-2), type="o", pch=20, xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors)
text(rep(-0, 9), beta[9,], colnames(x), pos=4, col=colors)
abline(v=lambda.lasso[4], lty=2)
abline(h=0, lty=2)
# It may help visualization if you plot using the scaled X
beta.scale <- attr(model.lasso$beta, "scaled:scale")
beta.rescaled <- beta
for(j in 1:9){
beta.rescaled[j,] <- beta.rescaled[j,]*beta.scale
}
# replot based on the scaled values.
matplot(lambda.lasso, beta.rescaled, xlim=c(8,-2), type="o", pch=20, xlab=expression(lambda),
ylab=expression(hat(beta)), col=colors)
text(rep(-0, 9), beta.rescaled[9,], colnames(x), pos=4, col=colors)
abline(v=lambda.lasso[4], lty=2)
abline(h=0, lty=2)
###############################################################################
# Build a Lasso using Glmnet
###############################################################################
lassoreg.cv <- cv.glmnet(x,y, alpha=1)
# Plot the fit
plot(lassoreg.cv)
# calculate the minimum lambda
lassoreg.cv$lambda.min
# The lambda value is 0.0328
###############################################################################
# Compute the lasso predictions
lassofits <- glmnet(x,y,alpha=1, nlambda=100)
# plot the fits
plot(lassofits)
plot(lassofits, xvar = "lambda", label = TRUE)
plot(lassofits, xvar = "dev", label = TRUE)
# Calculate the predictions with the minimal lambda
lassopred <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
# mydata$yhat <- predict(lassofits,x,s= lassoreg.cv$lambda.min)
# Here is how we find the coefficients for the Lasso.
lassocoef <- predict(lassofits,x,s=lassoreg.cv$lambda.min, type="coefficients")
lassocoef
# notice that the lcp was removed by the lasso (it has a value of 0)
# Lets calculate the prediction error.
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (lassopred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
# Therefore the error of the lasso is 0.46818
###############################################################################
# Build an elastic net using glmnet
###############################################################################
# alpha between 0 and 1 are examples of the elastic net.
# in this example we will use 0.2
elasticreg.cv <- cv.glmnet(x,y, alpha=0.2)
# Plot the fit
plot(elasticreg.cv)
# determine the minimum lambda
elasticreg.cv$lambda.min
###############################################################################
# Compute the elastic net predictions
elasticfits <- glmnet(x,y,alpha=0.2, nlambda=100)
# plot the fits
plot(elasticfits)
plot(elasticfits, xvar = "lambda", label = TRUE)
plot(elasticfits, xvar = "dev", label = TRUE)
# Calculate the predictions with the minimal lambda
elasticpred <- predict(elasticfits,x,s= elasticreg.cv$lambda.min)
# Here is how we find the coefficients for the Elastic.
elasticcoef <- predict(elasticfits,x,s=elasticreg.cv$lambda.min, type="coefficients")
elasticcoef
sum1 =0
tt<-nrow(mydata) # testset
for (i in 1:tt){
sum1 <- sum1 + (lassopred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
obj.lasso <- enet(x,y, lambda=0)
plot(obj.lasso, use.color=TRUE)
obj.enet <- enet(x,y, lambda=0.5)
plot(obj.enet, use.color=TRUE)
obj.cv <- cv.enet(x,y, lambda=0.5, s=seq(0,1, length=100),
mode = "fraction", trace = FALSE, max.steps=80)
mydata$elasticpred <- predict(elasticfits,x,s= elasticreg.cv$lambda.min)
View(mydata)
View(mydata)
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
boxplot(testout$X1, ylab="X1")
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), 'Yes','No')
library("outlier")
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), NA,testout$X1)
install.packages(outlier)
install.packages(Outlier)
install.packages("outlier")
library("outlier")
install.packages("outliers")
library("outliers")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), NA,testout$X1)
testout
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), 'No','Yes')
testout
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
testoutlm<-lm(Y~X1+X2,data=testout)
testout$fitted<-fitted(testoutlm)
testout$residual<-residuals(testoutlm)
testout$standardresid<-rstandard(testoutlm)
n<-nrow(testout)
q<-length(testoutlm$coefficients)
crit<-lundcrit(0.1,n,q)
testout$Ynew<-ifelse(abs(testout$standardresid)>crit,NA,testout$Y)
testout
crit<-lundcrit(0.5,n,q)
testout$Ynew<-ifelse(abs(testout$standardresid)>crit,NA,testout$Y)
testout
library("randomForest")
library("outliers")
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
testoutlm <- randomForest(Y ~ X1+X2, data=testout, ntree=50, mtry=3, importance=TRUE)
testoutlm <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
testout$fitted<-fitted(testoutlm)
testout$residual<-residuals(testoutlm)
testout$standardresid<-rstandard(testoutlm)
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
testout$residual <- testout$Y - predict(RFModel)
View(testout)
resid <- residuals(RFModel)
resid
library("outliers")
library("randomForest")
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
testout$residual <- testout$Y - predict(RFModel)
n<-nrow(testout)
q<-length(testoutlm$coefficients)
library("outliers")
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
testoutlm<-lm(Y~X1+X2,data=testout)
testout$fitted<-fitted(testoutlm)
testout$residual<-residuals(testoutlm)
testout$standardresid<-rstandard(testoutlm)
n<-nrow(testout)
testoutlm$coefficients
length(testoutlm$coefficients)
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
varUsed(RFModel)
length(varUsed(RFModel))
library("randomForest")
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
testout$residual <- testout$Y - predict(RFModel)
n<-nrow(testout)
q<-length(varUsed(RFModel))
crit<-lundcrit(0.1,n,q)
testout$Ynew<-ifelse(abs(testout$residual)>crit,NA,testout$Y)
testout
outlier(RFModel)
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
#Taint the data
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
residual <- testout$Y - predict(RFModel)
testout$residual <- testout$Y - predict(RFModel)
mean(residual)
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
residual <- testout$Y - predict(RFModel)
testout$residual <- testout$Y - predict(RFModel)
meanresid <- mean(residual)
OutlierThreshold <- 2
testout$Outlier <- residual[abs(residual-mean(residual))<OutlierThreshold*sd(residual)])
testout$Outlier <- residual[abs(residual-mean(residual))<OutlierThreshold*sd(residual)]
SD2 <- sd(residual)
SD2
SD2 <- OutlierThreshold*SDeviation
SDeviation <- sd(residual)
OutlierThreshold <- 2
SD2 <- OutlierThreshold*SDeviation
SD2
testout$Outlier<-ifelse(abs(testout$Resid)>SD2, 1, 0)
testout$Outlier<-ifelse(abs(testout$residual)>SD2, 1, 0)
View(testout)
plot(testout$residual, col=testout$Outlier+1, pch=16,ylim=c(-3,3))
plot(testout$residual, col=testout$Outlier+1, pch=16,ylim=c(-500,500))
testout2<-testout[!testout$Outlier,]
nrow(testout2)
plot(testout2$residual, col=testout2$Outlier+1, pch=16,ylim=c(-500,500))
iris2 <- iris[,1:4]
View(iris2)
View(iris2)
View(iris2)
iris2$Sepal.Length[10] <- 10
iris2$Sepal.Width[10] <- 10
iris2$Petal.Length[10] <- 10
iris2$Petal.Width[10] <- 10
View(iris2)
kmeans.result <- kmeans(iris2, centers=4)
kmeans.result$centers
kmeans.result <- kmeans(iris2, centers=3)
kmeans.result$centers
centers <- kmeans.result$centers[kmeans.result$cluster, ]
distances <- sqrt(rowSums((iris2 - centers)^2))
outliers <- order(distances, decreasing=T)[1:5]
print(outliers)
m <- tapply(distances, kmeans.result$cluster,mean)
d <- distances/(m[kmeans.result$cluster])
d[order(d, decreasing=TRUE)][1:5]
library("outliers")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
View(testout)
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
boxplot(testout$X1, ylab="X1")
boxplot(testout$X2, ylab="X2")
boxplot(testout$Y, ylab="Y")
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), 'Yes','No')
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
View(testout)
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
testoutlm<-lm(Y~X1+X2,data=testout)
testout$fitted<-fitted(testoutlm)
testout$residual<-residuals(testoutlm)
testout$standardresid<-rstandard(testoutlm)
n<-nrow(testout)
q<-length(testoutlm$coefficients)
crit<-lundcrit(0.1,n,q)
testout$Ynew<-ifelse(abs(testout$standardresid)>crit,NA,testout$Y)
testout
View(testout)
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
residual <- testout$Y - predict(RFModel)
residual <- testout$Y - predict(RFModel)
testout$residual <- testout$Y - predict(RFModel)
View(testout)
meanresid <- mean(residual)
SDeviation <- sd(residual)
OutlierThreshold <- 2
SD2 <- OutlierThreshold*SDeviation
testout$Outlier<-ifelse(abs(testout$residual)>SD2, 1, 0)
plot(testout$residual, col=testout$Outlier+1, pch=16,ylim=c(-500,500))
testout2<-testout[!testout$Outlier,]
nrow(testout2)
plot(testout2$residual, col=testout2$Outlier+1, pch=16,ylim=c(-500,500))
View(testout)
iris2 <- iris[,1:4]
View(iris2)
View(testout)
library("outliers")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), 'Yes','No')
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1),'Yes','No')
testout$newX1<-ifelse(testout$X1==outlier(testout$X1),NA,testout$X1)
setwd("C:/Users/Derek/Documents/RPackages/Sensor Data/")
library(caret)
library(h2o)
localH2O = h2o.init(ip = "localhost", port = 54321,
startH2O = TRUE, max_mem_size = '1g')
library(randomForest)
library(caret)
traindata<- read.csv("C:/Users/Derek/Documents/RPackages/Sensor Data/sensor_train.csv")
testdata<- read.csv("C:/Users/Derek/Documents/RPackages/Sensor Data/sensor_test.csv")
traindata$Class <- as.factor(traindata$Class)
RandomForestModel <- randomForest(Class ~., data=traindata, ntree=10, mtry=5, importance=TRUE)
print(RandomForestModel)
importance(RandomForestModel)
plot.new()
varImpPlot(RandomForestModel)
plot.new()
varImpPlot(RandomForestModel, type=1, pch=19, col=1, cex=1.0, main="")
abline(v=45, col="blue")
plot.new()
varImpPlot(RandomForestModel, type=2, pch=19, col=1, cex=1.0, main="")
abline(v=45, col="blue")
RFResult <- predict(RandomForestModel, testData, type="response")
RFResult <- predict(RandomForestModel, testdata, type="response")
testdata$YHat <- predict(RandomForestModel, testdata, type="response")
confusionMatrix(testdata$YHat, testdata$Class) # Random Forest
RandomForestModel <- randomForest(Class ~., data=traindata, ntree=100, mtry=5, importance=TRUE)
print(RandomForestModel)
importance(RandomForestModel)
plot.new()
varImpPlot(RandomForestModel, n.var=min(15, nrow(x$importance)))
plot.new()
varImpPlot(RandomForestModel, n.var=15)
plot.new()
varImpPlot(RandomForestModel, type=1, n.var= 15, pch=19, col=1, cex=1.0, main="")
abline(v=45, col="blue")
plot.new()
varImpPlot(RandomForestModel, type=2, n.var= 15, pch=19, col=1, cex=1.0, main="")
abline(v=45, col="blue")
RFResult <- predict(RandomForestModel, testdata, type="response")
testdata$YHat <- predict(RandomForestModel, testdata, type="response")
confusionMatrix(testdata$YHat, testdata$Class) # Random Forest
setwd("C:/Users/Derek/Documents/RPackages/Sensor Data/")
library(caret)
library(h2o)
localH2O = h2o.init(ip = "localhost", port = 54321,
startH2O = TRUE, max_mem_size = '1g')
train_h2o <- as.h2o(localH2O, traindata)
test_h2o <- as.h2o(localH2O, testdata)
model <- h2o.deeplearning(x = 1:561,  # column numbers for predictors
y = 562,   # column number for dependent variable
training_frame = train_h2o, # train data in H2O format
validation_frame = test_h2o, # test data in H20 Format
activation = "TanhWithDropout", # or 'Tanh'
input_dropout_ratio = 0.2, # % of inputs dropout
hidden_dropout_ratios = c(0.5,0.5,0.5), # % for nodes dropout
balance_classes = TRUE,
hidden = c(50,50,50), # three layers of 50 nodes
epochs = 10) # max. no. of epochs
model
perf <- h2o.performance(model, train_h2o)
h2o.accuracy(perf)
model@model$params
model@model$varimp
model@model$confusion
model@model$params
model@model$params
h2o_Yhat <- h2o.predict(model, test_h2o)
df_yhat_test <- as.data.frame(h2o_Yhat)
View(df_yhat_test)
mydata <- cbind(testdata, df_yhat_test)
View(df_yhat_test)
confusionMatrix(mydata$predict, mydata$Class) # Random Forest
model <- h2o.deeplearning(x = 1:561,  # column numbers for predictors
y = 562,   # column number for label
training_frame = train_h2o, # train data in H2O format
validation_frame = test_h2o, # test data in H20 Format
activation = "RectifierWithDropout",
hidden = c(50,50,50), # three layers of 1024,1024,2048 nodes
epochs = 4, # max. no. of epochs - 8000
l1 = 1e-5,
input_dropout_ratio = 0.2, # % of inputs dropout
train_samples_per_iteration = -1,
classification_stop = -1)
model
model <- h2o.deeplearning(x = 1:561,  # column numbers for predictors
y = 562,   # column number for label
training_frame = train_h2o, # train data in H2O format
validation_frame = test_h2o, # test data in H20 Format
activation = "RectifierWithDropout",
hidden = c(50,50,50), # three layers of 1024,1024,2048 nodes
epochs = 12, # max. no. of epochs - 8000
l1 = 1e-5,
input_dropout_ratio = 0.2, # % of inputs dropout
train_samples_per_iteration = -1,
classification_stop = -1)
model
model <- h2o.deeplearning(x = 1:561,  # column numbers for predictors
y = 562,   # column number for dependent variable
training_frame = train_h2o, # train data in H2O format
validation_frame = test_h2o, # test data in H20 Format
activation = "TanhWithDropout", # or 'Tanh'
input_dropout_ratio = 0.2, # % of inputs dropout
hidden_dropout_ratios = c(0.5,0.5,0.5), # % for nodes dropout
balance_classes = TRUE,
hidden = c(50,50,50), # three layers of 50 nodes
epochs = 100) # max. no. of epochs
model
h2o_Yhat <- h2o.predict(model, test_h2o)
## Converting H2O format into data frame
df_yhat_test <- as.data.frame(h2o_Yhat)
# merge the dataframes for prediction
mydata <- cbind(testdata, df_yhat_test)
confusionMatrix(mydata$predict, mydata$Class) # Random Forest
traindata$Class <- as.factor(traindata$Class)
RandomForestModel <- randomForest(Class ~., data=traindata, ntree=100, mtry=5, importance=TRUE)
RFResult <- predict(RandomForestModel, testdata, type="response")
testdata$YHat <- predict(RandomForestModel, testdata, type="response")
testdata$YHat <- predict(RandomForestModel, testdata, type="response")
confusionMatrix(testdata$YHat, testdata$Class) # Random Forest
View(traindata)
