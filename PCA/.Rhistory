some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
########################################################################
# Perform Sentiment Analysis.
# classify emotion
class_emo = classify_emotion(some_txt, algorithm="bayes", prior=1.0)
# get emotion best fit
emotion = class_emo[,7]
# substitute NA's by "unknown"
emotion[is.na(emotion)] = "unknown"
# classify polarity
class_pol = classify_polarity(some_txt, algorithm="bayes")
# get polarity best fit
polarity = class_pol[,4]
########################################################################
# Create a data frame with the results.
sent_df = data.frame(SearchTerm=Var1, created=created, text=some_txt, emotion=emotion,
polarity=polarity, screenName=screenName,
retweetCount=retweetCount, stringsAsFactors=FALSE)
sent_df = data.frame(SearchTerm=Var1, created=created, text=some_txt, emotion=emotion,
polarity=polarity, screenName=screenName, stringsAsFactors=FALSE)
library("ROAuth")
library("twitteR")
library("wordcloud")
library("tm")
library("plyr")
library("stringr")
library("ggplot2")
library("sentiment")
library("RColorBrewer")
# This step can be run first if we have already authenticated our twitter
# account.
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
########################################################################
# Get tweets from twitter.
Var1 <- '#ChicagoBulls'
Tweets <- searchTwitter(Var1, n=50, cainfo="cacert.pem")
Tweets.df <- twListToDF(Tweets)
# Sentiment Analysis
########################################################################
# get the text from the tweets
some_txt = sapply(Tweets, function(x) x$getText())
########################################################################
# Prepare the tweets for text analysis.
# remove retweet entities
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
# remove at people
some_txt = gsub("@\\w+", "", some_txt)
# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt = gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
# define "tolower error handling" function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
########################################################################
# Perform Sentiment Analysis.
# classify emotion
class_emo = classify_emotion(some_txt, algorithm="bayes", prior=1.0)
# get emotion best fit
emotion = class_emo[,7]
# substitute NA's by "unknown"
emotion[is.na(emotion)] = "unknown"
# classify polarity
class_pol = classify_polarity(some_txt, algorithm="bayes")
# get polarity best fit
polarity = class_pol[,4]
########################################################################
# Create a data frame with the results.
sent_df = data.frame(SearchTerm=Var1, created=created, text=some_txt, emotion=emotion,
polarity=polarity, screenName=screenName, stringsAsFactors=FALSE)
View(Tweets.df)
sent_df = data.frame(SearchTerm=Var1, text=some_txt, emotion=emotion,
polarity=polarity, screenName=screenName, stringsAsFactors=FALSE)
sent_df = data.frame(text=some_txt, emotion=emotion,
polarity=polarity, screenName=screenName, stringsAsFactors=FALSE)
sent_df = data.frame(text=some_txt, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
View(sent_df)
sent_df$SearchTerm <- Var1
View(Tweets.df)
sent_df$Date <- Tweets.df$created
View(sent_df)
View(Tweets.df)
sent_df$ScreenName <- Tweets.df$screenName
View(sent_df)
View(Tweets.df)
library(Rfacebook)
library(Rook)
#so if you want to connect to Facebook again you just have to call
load("fb_oauth")
Friends <- getFriends(token=fb_oauth, simplify = FALSE)
View(Friends)
my_friends_info <- getUsers(my_friends$id, token=fb_oauth, private_info=TRUE)
my_friends_info <- getUsers(Friends$id, token=fb_oauth, private_info=TRUE)
View(my_friends_info)
Checkin <-getCheckins(Friends$id, n=10, token=fb_oauth, tags = FALSE)
View(Friends)
Checkin <-getCheckins(user="1075852970", n=10, token=fb_oauth, tags = FALSE)
Checkin <-getCheckins(user="1075852970", n=10, token=fb_oauth, tags = FALSE)
Likes <-getLikes(user="1075852970", n=500, token=fb_oauth)
View(Likes)
Likes <-getLikes(Friends$id, n=500, token=fb_oauth)
NewsFeed <- getNewsfeed(token=fb_oauth, n=500)
View(NewsFeed)
Likes <-getLikes(user="1075852970", n=500, token=fb_oauth)
View(Likes)
Page <- getPage(page="Fox Valley Concert Band", token=fb_oauth, n=100, feed=FALSE)
getPage(page="Fox Valley Concert Band", token=fb_oauth, n=100, feed=FALSE)
Page <- getPage(page="Woodford Reserve", token=fb_oauth, n=100, feed=FALSE)
fb_page <- getPage(page="facebook", token=fb_oauth)
View(fb_page)
fb_page <- getPage(page="Woodford Reserve", token=fb_oauth)
fb_page <- getPage(page="WoodfordReserve", token=fb_oauth)
View(fb_page)
fb_page <- getPage(page="FoxValleyConcertBand", token=fb_oauth)
View(fb_page)
posts <- searchFacebook(string ="ebola", token=fb_oauth, n = 500)
library(Rfacebook)
library(Rook)
library("tm")
library("plyr")
library("class")
load("fb_oauth")
Friends <- getFriends(token=fb_oauth, simplify = FALSE)
Likes <-getLikes(user="1075852970", n=500, token=fb_oauth)
View(Likes)
Likes$User <- "1075852970"
View(Likes)
View(Friends)
NewsFeed <- getNewsfeed(token=fb_oauth, n=500)
View(Friends)
NewsFeed$User <- "123456789"
View(NewsFeed)
Page <- getPage(page="FoxValleyConcertBand", token=fb_oauth)
library("ROAuth")
library("twitteR")
library("wordcloud")
library("tm")
library("plyr")
library("stringr")
library("ggplot2")
library("sentiment")
library("RColorBrewer")
# This step can be run first if we have already authenticated our twitter
# account.
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
########################################################################
# Get tweets from twitter.
Var1 <- '#ChicagoBulls'
Tweets <- searchTwitter(Var1, n=50, cainfo="cacert.pem")
Tweets.df <- twListToDF(Tweets)
# Sentiment Analysis
########################################################################
# get the text from the tweets
some_txt = sapply(Tweets, function(x) x$getText())
########################################################################
# Prepare the tweets for text analysis.
# remove retweet entities
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)
# remove at people
some_txt = gsub("@\\w+", "", some_txt)
# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt = gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)
# define "tolower error handling" function
try.error = function(x)
{
# create missing value
y = NA
# tryCatch error
try_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(try_error, "error"))
y = tolower(x)
# result
return(y)
}
# lower case using try.error with sapply
some_txt = sapply(some_txt, try.error)
# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL
########################################################################
# Perform Sentiment Analysis.
# classify emotion
class_emo = classify_emotion(some_txt, algorithm="bayes", prior=1.0)
# get emotion best fit
emotion = class_emo[,7]
# substitute NA's by "unknown"
emotion[is.na(emotion)] = "unknown"
# classify polarity
class_pol = classify_polarity(some_txt, algorithm="bayes")
# get polarity best fit
polarity = class_pol[,4]
########################################################################
# Create a data frame with the results.
sent_df = data.frame(text=some_txt, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
sent_df$SearchTerm <- Var1
sent_df$Date <- Tweets.df$created
sent_df$ScreenName <- Tweets.df$screenName
########################################################################
write.csv(sent_df, file='C:/Business Intelligence/Twitter/Tweets.csv', row.names=F)
library(Rfacebook)
library(Rook)
library("tm")
library("plyr")
library("class")
#so if you want to connect to Facebook again you just have to call
load("fb_oauth")
########################################################################
Friends <- getFriends(token=fb_oauth, simplify = FALSE)
########################################################################
Likes <-getLikes(user="1075852970", n=500, token=fb_oauth)
Likes$User <- "1075852970"
########################################################################
NewsFeed <- getNewsfeed(token=fb_oauth, n=500)
NewsFeed$User <- "123456789"
########################################################################
Page <- getPage(page="FoxValleyConcertBand", token=fb_oauth)
########################################################################
write.csv(Page, file='C:/Business Intelligence/Facebook/Page.csv', row.names=F)
write.csv(NewsFeed, file='C:/Business Intelligence/Facebook/NewsFeed.csv', row.names=F)
write.csv(Likes, file='C:/Business Intelligence/Facebook/Likes.csv', row.names=F)
write.csv(Friends, file='C:/Business Intelligence/Facebook/Friends.csv', row.names=F)
library("ROAuth")
library("twitteR")
library("wordcloud")
library("tm")
library("plyr")
library("stringr")
library("ggplot2")
library("sentiment")
library("RColorBrewer")
# This step can be run first if we have already authenticated our twitter
# account.
load("twitter authentication.Rdata")
registerTwitterOAuth(Cred)
########################################################################
# Get tweets from twitter.
Var1 <- '#Bulls'
Tweets <- searchTwitter(Var1, n=50, cainfo="cacert.pem")
Tweets.df <- twListToDF(Tweets)
y <- c(“abc”, “abcdef”, “a”, “abcd”)
sapply(y, nchar)
y <- c("abc", "abcdef", "a", "abcd")
sapply(y, nchar)
sum <- 1
repeat{
sum <- sum + 2;
print(sum);
if (sum > 11)
break;
}
x<-0;
while (x < 10)
{
x<- x+4;
print (x);
}
library("openNLP")
x=seq(1,5,0.5)
y=3*x^2
plot(x,y)
plot(log(x),log(y))
res <- lm(log(y) ~ log(x))
res
log(3)
res <- lm(log(y) ~ log(x))
res
res <- lm(log(y) ~ log(x))
res
log(3)
x2 <- log(x)
y2 <- log(y)
plot(x2,y2)
df <- rbind(x2,y2)
View(df)
View(df)
primates=read.table(file=file.choose(),header=TRUE)
library("dataset")
library("datasets")
mydata <- cars
View(mydata)
plot(RMR~Weight,data=primates)
x=seq(1,5,0.5)
y=3*2^x
plot(x,y)
setwd("C:/Users/Derek/Documents/RPackages/Exploratory Data Analysis")
mydata <- read.csv("EDA.csv")
reg1 <- lm(Y ~ X, data=mydata)
summary(reg1)
reg1 <- lm(Y ~ sqrt(X), data=mydata)
summary(reg1)
reg1 <- lm(Y ~ X + 0, data=mydata)
summary(reg1)
View(mydata)
mydata <- read.csv("EDA.csv")
reg1 <- lm(Y ~ X + 0, data=mydata)
summary(reg1)
reg1 <- lm(Y ~ sqrt(X) + 0, data=mydata)
summary(reg1)
reg1 <- lm(Y ~ X^2 + 0, data=mydata)
summary(reg1)
View(mydata)
mydata$Y2 <- sqrt(Y)
mydata$Y2 <- sqrt(mydata$Y)
reg1 <- lm(Y2 ~ X + 0, data=mydata)
summary(reg1)
library(ggplot2)
chart1 <- plot(X,Y)
chart1 <- plot(mydata$X,mydata$Y)
lm1 <- lm(Y~X, data=mydata)
abline(coefficients(lm1))
mydata$Y2 <- sqrt(mydata$Y)
chart2 <- plot(mydata$X,mydata$Y2)
reg1 <- lm(Y2 ~ X + 0, data=mydata)
abline(coefficients(reg1))
reg1 <- lm(Y2 ~ X, data=mydata)
abline(coefficients(reg1))
mydata$Y2 <- mydata$Y^1/2
chart2 <- plot(mydata$X,mydata$Y2)
mydata$Y2 <- mydata$Y^0.5
chart2 <- plot(mydata$X,mydata$Y2)
mydata$Y2 <- mydata$Y^(1/2)
chart2 <- plot(mydata$X,mydata$Y2)
summary(reg1)
library(ggplot2)
chart1 <- plot(mydata$X,mydata$Y)
lm1 <- lm(Y~X, data=mydata)
abline(coefficients(lm1))
library(MASS)
boxcox(lm1)
boxcox(lm1, lambda=seq(1,2,0.1))
boxcox(lm1)
boxcox(lm1, lambda=seq(0,1,0.1))
setwd("C:/Users/Derek/Documents/RPackages/Exploratory Data Analysis")
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Load the dataset
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
mydata <- read.csv("House.csv")
mydata <- read.csv("Housing.csv")
View(mydata)
mydata$X <- NULL
View(mydata)
library(car)
mydata2 <- Prestige
setwd("C:/Users/kane.de/Documents/RPackages/Regression Tutorial")
setwd("C:/Users/Derek/Documents/RPackages/Regression Tutorial")
reg1 <- lm(prestige ~ education + log2(income) + women, data=Prestige)
summary(reg1)
prestige_hat<-fitted(reg1) # predicted values
as.data.frame(prestige_hat)
prestige_resid<-residuals(reg1) # residuals
as.data.frame(prestige_resid)
reg2 <- lm(prestige ~ education + log2(income) + type, data=Prestige)
summary(reg2)
Prestige$type<-with(Prestige, factor(type, levels=c("bc", "wc", "prof")))
reg3 <- lm(prestige ~ type*(education + log2(income)), data=Prestige)
summary(reg3)
reg1 <- lm(prestige ~ education + log2(income) + women, data=Prestige)
residualPlots(reg1)
residualPlots(reg1, ~ 1, fitted=TRUE) #Residuals vs fitted only
residualPlots(reg1, ~ education, fitted=FALSE) # Residuals vs education only
reg4 <-lm(prestige ~ education + income + type, data = Prestige)
avPlots(reg4, id.n=2, id.cex=0.7)
reg4 <-lm(prestige ~ education + income + type, data = Prestige)
qqPlot(reg4, id.n=3)
reg4 <-lm(prestige ~ education + income + type, data = Prestige)
outlierTest(reg4)
reg4 <-lm(prestige ~ education + income + type, data = Prestige)
influenceIndexPlot(reg1, id.n=3)
qqPlot(reg4, id.n=3)
influenceIndexPlot(reg1, id.n=3)
avPlots(reg4, id.n=2, id.cex=0.7)
qqPlot(reg4)
library(ggplot2)
# We need to establish the working directory where the file is located within.
# set the working directory
setwd("C:/Users/Derek/Documents/RPackages/GG Plot Examples")
filepath<-"http://bit.ly/wBBTQO"
myData<-read.table(file=url(filepath),header=T,sep="\t")
str(myData)
qplot(data=myData,x=BM,main="Histogram of BodyMass")
qplot(data=myData,x=Hab,y=var1)
qplot(data=myData,x=BM,y=var1,log="xy",color=Tribe)
setwd("C:/Users/dkane/Documents/R Packages/Marion County")
# Import the data from csv file
mydata<- read.csv("C:/Users/dkane/Documents/R Packages/Marion County/recidivism4e2.csv")
attach(mydata)
setwd ("C:/Users/dkane/Documents/R Packages/Predictive Model Example")
setwd ("C:/Users/Derek/Documents/RPackages/Predictive Model Example")
library(XLConnect)
require(XLConnect)
wb = loadWorkbook("Iris Data.xlsx")
df = readWorksheet(wb, sheet = "Sheet1", header = TRUE)
mydata = readWorksheet(wb, sheet = "Sheet1", header = TRUE)
head(mydata, n=10)
str(mydata)
mcor<-cor(mydata)
mydata2 <- mydata
mydata2$Classification <- NULL
mcor<-cor(mydata2)
round(mcor, digits=2)
corrplot(mcor)
library(aod)
library(ggplot2)
library(reshape2)
library(car)
library(corrplot)
install.packages(corrplot)
install.packages("corrplot")
corrplot(mcor)
library(corrplot)
corrplot(mcor)
qplot(x=Var1, y=Var2, data=melt(cor(mydata2, use="p")),
fill=value, geom="tile") +   scale_fill_gradient2(limits=c(-1, 1))
plot(mydata)
plot(mydata2)
plotmatrix(mydata2, colour="gray20")
plotmatrix(mydata2, colour="royalblue2")
plotmatrix(mydata2, colour="turquoise2")
plotmatrix(mydata2, colour="turquoise2") + geom_smooth(method="lm")
plotmatrix(PW ~ SL, colour="turquoise2") + geom_smooth(method="lm")
plotmatrix(mydata2$PW ~mydata2$SL, colour="turquoise2") + geom_smooth(method="lm")
plot(mydata2$PW ~mydata2$SL)
ggplot(mydata2, aes(x=xvar, y=yvar, color=cond)) + geom_point(shape=1) +
scale_colour_hue(l=50) + # Use a slightly darker palette than normal
geom_smooth(method=lm,   # Add linear regression lines
se=FALSE,    # Don't add shaded confidence region
fullrange=T) # Extend regression lines
# Extend the regression lines beyond the domain of the data
ggplot(mydata2, aes(x=mydata2$PW, y=mydata2$SL, color=cond)) + geom_point(shape=1) +
scale_colour_hue(l=50) + # Use a slightly darker palette than normal
geom_smooth(method=lm,   # Add linear regression lines
se=FALSE,    # Don't add shaded confidence region
fullrange=T) # Extend regression lines
ggplot(mydata, aes(x=mydata$PW, y=mydata$SL, color=mydata$Classification)) + geom_point(shape=1) +
scale_colour_hue(l=50) + # Use a slightly darker palette than normal
geom_smooth(method=lm,   # Add linear regression lines
se=FALSE,    # Don't add shaded confidence region
fullrange=T) # Extend regression lines
ggplot(mydata, aes(x=mydata$PW, y=mydata$SL, color=mydata$Classification)) + geom_point(shape=1) +
scale_colour_hue(l=50)
ggplot(mydata, aes(x=mydata$PW, y=mydata$SL, color=mydata$Classification))
ggplot(mydata, aes(x=mydata$PW, y=mydata$SL, color=mydata$Classification)) +
scale_colour_hue(l=50)
ggplot(mydata, aes(x=mydata$PW, y=mydata$SL, color=mydata$Classification)) + geom_point(shape=0) +
scale_colour_hue(l=50) + # Use a slightly darker palette than normal
ggplot(mydata, aes(x=mydata$PW, y=mydata$SL, color=mydata$Classification)) + geom_point(shape=2) +
scale_colour_hue(l=50) + # Use a slightly darker palette than normal
ggplot(mydata, aes(x=mydata$PW, y=mydata$SL, color=mydata$Classification)) + geom_point(shape=2) +
scale_colour_hue(l=50)
lines beyond the domain of the data
ggplot(mydata, aes(x=mydata$PW, y=mydata$SL, color=mydata$Classification)) + geom_point(shape=0) +
scale_colour_hue(l=50)
lines beyond the domain of the data
ggplot(mydata, aes(x=mydata$PW, y=mydata$SL, color=mydata$Classification)) + geom_point(shape=3) +
scale_colour_hue(l=50)
ggplot(mydata, aes(x=mydata$PW, y=mydata$SL, color=mydata$Classification)) + geom_point(shape=4) +
scale_colour_hue(l=50)
ggplot(mydata, aes(x=mydata$PW, y=mydata$SL, color=mydata$Classification)) + geom_point(shape=5) +
scale_colour_hue(l=50)
setwd("C:/Users/dkane/Documents/R Packages/PCA")
setwd("C:/Users/Kane/Documents/R Packages/PCA")
setwd("C:/Users/Derek/Documents/RPackages/PCA")
setwd("C:/Users/Derek/Documents/RPackages/PCA")
# Import the data from csv file
mydata<- read.csv("C:/Users/dkane/Documents/R Packages/PCA/PCA.csv")
attach(mydata)
mydata2 <- mydata
setwd("C:/Users/Derek/Documents/RPackages/PCA")
mydata<- read.csv("C:/Users/Derek/Documents/RPackages/PCA/PCA.csv")
attach(mydata)
mydata2 <- mydata
View(mydata)
mydata$Participant <- NULL
mydata$OS<- NULL
View(mydata)
pca <- princomp(mydata, cor=T)
summary(pca, loadings=T)
