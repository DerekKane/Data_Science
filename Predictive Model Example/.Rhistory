for (i in 1:tt){
sum1 <- sum1 + (lassopred[i]-mydata[i,9])^2
}
sumg <- sum1/tt
sumg
obj.lasso <- enet(x,y, lambda=0)
plot(obj.lasso, use.color=TRUE)
obj.enet <- enet(x,y, lambda=0.5)
plot(obj.enet, use.color=TRUE)
obj.cv <- cv.enet(x,y, lambda=0.5, s=seq(0,1, length=100),
mode = "fraction", trace = FALSE, max.steps=80)
mydata$elasticpred <- predict(elasticfits,x,s= elasticreg.cv$lambda.min)
View(mydata)
View(mydata)
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
boxplot(testout$X1, ylab="X1")
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), 'Yes','No')
library("outlier")
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), NA,testout$X1)
install.packages(outlier)
install.packages(Outlier)
install.packages("outlier")
library("outlier")
install.packages("outliers")
library("outliers")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), NA,testout$X1)
testout
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), 'No','Yes')
testout
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
testoutlm<-lm(Y~X1+X2,data=testout)
testout$fitted<-fitted(testoutlm)
testout$residual<-residuals(testoutlm)
testout$standardresid<-rstandard(testoutlm)
n<-nrow(testout)
q<-length(testoutlm$coefficients)
crit<-lundcrit(0.1,n,q)
testout$Ynew<-ifelse(abs(testout$standardresid)>crit,NA,testout$Y)
testout
crit<-lundcrit(0.5,n,q)
testout$Ynew<-ifelse(abs(testout$standardresid)>crit,NA,testout$Y)
testout
library("randomForest")
library("outliers")
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
testoutlm <- randomForest(Y ~ X1+X2, data=testout, ntree=50, mtry=3, importance=TRUE)
testoutlm <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
testout$fitted<-fitted(testoutlm)
testout$residual<-residuals(testoutlm)
testout$standardresid<-rstandard(testoutlm)
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
testout$residual <- testout$Y - predict(RFModel)
View(testout)
resid <- residuals(RFModel)
resid
library("outliers")
library("randomForest")
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
testout$residual <- testout$Y - predict(RFModel)
n<-nrow(testout)
q<-length(testoutlm$coefficients)
library("outliers")
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
testoutlm<-lm(Y~X1+X2,data=testout)
testout$fitted<-fitted(testoutlm)
testout$residual<-residuals(testoutlm)
testout$standardresid<-rstandard(testoutlm)
n<-nrow(testout)
testoutlm$coefficients
length(testoutlm$coefficients)
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
varUsed(RFModel)
length(varUsed(RFModel))
library("randomForest")
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
testout$residual <- testout$Y - predict(RFModel)
n<-nrow(testout)
q<-length(varUsed(RFModel))
crit<-lundcrit(0.1,n,q)
testout$Ynew<-ifelse(abs(testout$residual)>crit,NA,testout$Y)
testout
outlier(RFModel)
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
#Taint the data
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
residual <- testout$Y - predict(RFModel)
testout$residual <- testout$Y - predict(RFModel)
mean(residual)
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
residual <- testout$Y - predict(RFModel)
testout$residual <- testout$Y - predict(RFModel)
meanresid <- mean(residual)
OutlierThreshold <- 2
testout$Outlier <- residual[abs(residual-mean(residual))<OutlierThreshold*sd(residual)])
testout$Outlier <- residual[abs(residual-mean(residual))<OutlierThreshold*sd(residual)]
SD2 <- sd(residual)
SD2
SD2 <- OutlierThreshold*SDeviation
SDeviation <- sd(residual)
OutlierThreshold <- 2
SD2 <- OutlierThreshold*SDeviation
SD2
testout$Outlier<-ifelse(abs(testout$Resid)>SD2, 1, 0)
testout$Outlier<-ifelse(abs(testout$residual)>SD2, 1, 0)
View(testout)
plot(testout$residual, col=testout$Outlier+1, pch=16,ylim=c(-3,3))
plot(testout$residual, col=testout$Outlier+1, pch=16,ylim=c(-500,500))
testout2<-testout[!testout$Outlier,]
nrow(testout2)
plot(testout2$residual, col=testout2$Outlier+1, pch=16,ylim=c(-500,500))
iris2 <- iris[,1:4]
View(iris2)
View(iris2)
View(iris2)
iris2$Sepal.Length[10] <- 10
iris2$Sepal.Width[10] <- 10
iris2$Petal.Length[10] <- 10
iris2$Petal.Width[10] <- 10
View(iris2)
kmeans.result <- kmeans(iris2, centers=4)
kmeans.result$centers
kmeans.result <- kmeans(iris2, centers=3)
kmeans.result$centers
centers <- kmeans.result$centers[kmeans.result$cluster, ]
distances <- sqrt(rowSums((iris2 - centers)^2))
outliers <- order(distances, decreasing=T)[1:5]
print(outliers)
m <- tapply(distances, kmeans.result$cluster,mean)
d <- distances/(m[kmeans.result$cluster])
d[order(d, decreasing=TRUE)][1:5]
library("outliers")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
View(testout)
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
boxplot(testout$X1, ylab="X1")
boxplot(testout$X2, ylab="X2")
boxplot(testout$Y, ylab="Y")
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), 'Yes','No')
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
View(testout)
lundcrit<-function(a, n, q) {
# Calculates a Critical value for Outlier Test according to Lund
# See Lund, R. E. 1975, "Tables for An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 4, pp. 473-476.
# and Prescott, P. 1975, "An Approximate Test for Outliers in Linear Models", Technometrics, vol. 17, no. 1, pp. 129-132.
# a = alpha
# n = Number of data elements
# q = Number of independent Variables (including intercept)
F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE)
crit<-((n-q)*F/(n-q-1+F))^0.5
crit
}
testoutlm<-lm(Y~X1+X2,data=testout)
testout$fitted<-fitted(testoutlm)
testout$residual<-residuals(testoutlm)
testout$standardresid<-rstandard(testoutlm)
n<-nrow(testout)
q<-length(testoutlm$coefficients)
crit<-lundcrit(0.1,n,q)
testout$Ynew<-ifelse(abs(testout$standardresid)>crit,NA,testout$Y)
testout
View(testout)
library("randomForest")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
RFModel <- randomForest(Y ~ X1+X2, data=testout, ntree=25, mtry=2, importance=TRUE)
residual <- testout$Y - predict(RFModel)
residual <- testout$Y - predict(RFModel)
testout$residual <- testout$Y - predict(RFModel)
View(testout)
meanresid <- mean(residual)
SDeviation <- sd(residual)
OutlierThreshold <- 2
SD2 <- OutlierThreshold*SDeviation
testout$Outlier<-ifelse(abs(testout$residual)>SD2, 1, 0)
plot(testout$residual, col=testout$Outlier+1, pch=16,ylim=c(-500,500))
testout2<-testout[!testout$Outlier,]
nrow(testout2)
plot(testout2$residual, col=testout2$Outlier+1, pch=16,ylim=c(-500,500))
View(testout)
iris2 <- iris[,1:4]
View(iris2)
View(testout)
library("outliers")
testout <- data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5), Y=rnorm(50,mean=200,sd=25))
testout$X1[10] <- 5
testout$X2[10] <- 5
testout$Y[10] <- 530
testout
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1), 'Yes','No')
testout$newX1 <- ifelse(testout$X1==outlier(testout$X1),'Yes','No')
testout$newX1<-ifelse(testout$X1==outlier(testout$X1),NA,testout$X1)
library(caret)
library(h2o)
localH2O = h2o.init(ip = "localhost", port = 54321,
startH2O = TRUE, max_mem_size = '1g')
traindata<- read.csv("/Deep Learning/mnist_train.csv")
testdata<- read.csv("/Deep Learning/mnist_test.csv")
traindata<- read.csv("C:/Users/Derek/Documents/RPackages/Deep Learning/mnist_train.csv")
testdata<- read.csv("C:/Users/Derek/Documents/RPackages/Deep Learning/mnist_test.csv")
traindata$X5 <- as.factor(traindata$X5)
testdata$X7 <- as.factor(testdata$X7)
train_h2o <- as.h2o(localH2O, traindata)
test_h2o <- as.h2o(localH2O, testdata)
model <- h2o.deeplearning(x = 2:785,  # column numbers for predictors
y = 1,   # column number for dependent variable
training_frame = train_h2o, # train data in H2O format
validation_frame = test_h2o, # test data in H20 Format
activation = "TanhWithDropout", # or 'Tanh'
input_dropout_ratio = 0.2, # % of inputs dropout
hidden_dropout_ratios = c(0.5,0.5,0.5), # % for nodes dropout
balance_classes = TRUE,
hidden = c(50,50,50), # three layers of 50 nodes
epochs = 10) # max. no. of epochs
testdata$X5 <- as.factor(traindata$X5)
testdata$X5 <- as.factor(testdata$X5)
test_h2o <- as.h2o(localH2O, testdata)
model <- h2o.deeplearning(x = 2:785,  # column numbers for predictors
y = 1,   # column number for dependent variable
training_frame = train_h2o, # train data in H2O format
validation_frame = test_h2o, # test data in H20 Format
activation = "TanhWithDropout", # or 'Tanh'
input_dropout_ratio = 0.2, # % of inputs dropout
hidden_dropout_ratios = c(0.5,0.5,0.5), # % for nodes dropout
balance_classes = TRUE,
hidden = c(50,50,50), # three layers of 50 nodes
epochs = 10) # max. no. of epochs
setwd("C:C:/Users/Derek/Dropbox/DKane R Tutorial/Decision Trees/R Example/")
# Load Libraries
library(caret)
library(rpart)
library(rpart.plot)
library(C50)
library(rattle)
library(party)
library(partykit)
library(RWeka)
library(randomForest)
library(ROCR)
# Load the data
diabetes <- read.csv("Diabetes.csv")
churn <- read.csv("Churn.csv")
setwd("C:/Users/Derek/Documents/RPackages/Predictive Model Example")
library(ggplot2)
library(reshape2)
library(car)
library(corrplot)
library(e1071)
library(randomForest)
library(ROCR)
library(caret)
mydata <- read.csv("breastcancer.csv")
attach(mydata)
View(mydata)
summary(mydata)
mydata$Sample.Code <- NULL
Values <- c("1","2","3","4","5","6","7","8","9","10")
mydata <- mydata[mydata$Bare.Nuclei %in% Values,]
# Convert the Bare.Nuclei variable to integer from factor
mydata$Bare.Nuclei <- as.integer(mydata$Bare.Nuclei)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# the class variable (0 or 1) need to be converted into catgorical values
# Reencode variable from 0 to 1 to Cancer or No Cancer.
mydata$Class[mydata$Class=="0"] <- "No Cancer"
mydata$Class[mydata$Class=="1"] <- "Cancer"
# And Back again...
mydata$Class[mydata$Class=="No Cancer"] <- 0
mydata$Class[mydata$Class=="Cancer"] <- 1
mydata$Class <- as.integer(mydata$Class)
na.omit(mydata)
summary(mydata)
sapply(mydata, sd)
# Correlation Matrix
cormatrix  <- round(cor(mydata), digits=2)
corrplot(cormatrix)
qplot(x=Var1, y=Var2, data=melt(cor(mydata, use="p")),
fill=value, geom="tile") +   scale_fill_gradient2(limits=c(-1, 1))
set.seed(1234)
ind <- sample(2, nrow(mydata), replace=TRUE, prob=c(0.7, 0.3))
trainData <- mydata[ind==1,]
testData <- mydata[ind==2,]
forwardtest <- step(glm(Class~1, data = trainData), direction="forward", scope=~ Clump.Thickness + Uniformity.of.Cell.Size + Uniformity.of.Cell.Shape + Marginal.Adhesion +
Single.Epithelial.Cell.Size + Bare.Nuclei + Bland.Chromatin + Normal.Nucleoli + Mitoses )
mylogit <- glm(Class ~ Uniformity.of.Cell.Shape + Clump.Thickness + 0, data = trainData, family = "binomial")
summary(mylogit)
residualPlots(mylogit, layout=c(1, 3))
# influence Plot
influenceIndexPlot(mylogit, vars=c("Cook", "hat"), id.n=3)
confint(mylogit)
## CIs using standard errors
confint.default(mylogit)
exp(mylogit$coefficients)
exp(confint(mylogit))
## odds ratios only
exp(coef(mylogit))
## odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))
tuned <- tune.svm(Class ~ Clump.Thickness + Uniformity.of.Cell.Size + Uniformity.of.Cell.Shape
+ Marginal.Adhesion + Single.Epithelial.Cell.Size + Bare.Nuclei
+ Bland.Chromatin + Normal.Nucleoli + Mitoses,
data = trainData, gamma = 10^(-6:-1), cost = 10^(-1:1))
summary(tuned)
SVMModel <- svm(Class ~ Clump.Thickness + Uniformity.of.Cell.Size +
Uniformity.of.Cell.Shape+ Marginal.Adhesion +
Single.Epithelial.Cell.Size + Bare.Nuclei +
Bland.Chromatin + Normal.Nucleoli + Mitoses,
data = trainData, gamma=0.1, cost=1)
print(SVMModel)
summary(SVMModel)
RandomForestModel <- randomForest(Class ~ Clump.Thickness + Uniformity.of.Cell.Size +
Uniformity.of.Cell.Shape+ Marginal.Adhesion +
Single.Epithelial.Cell.Size + Bare.Nuclei +
Bland.Chromatin + Normal.Nucleoli + Mitoses,
data=trainData, ntree=500, mtry=5, importance=TRUE)
print(RandomForestModel)
importance(RandomForestModel)
print(RandomForestModel)
importance(RandomForestModel)
plot.new()
varImpPlot(RandomForestModel)
plot.new()
varImpPlot(RandomForestModel, type=2, pch=19, col=1, cex=1.0, main="")
abline(v=20, col="blue")
LogisticModel <- predict(mylogit, testData, type="response")
SVMResult <- predict(SVMModel, testData, type="response")
RFResult <- predict(RandomForestModel, testData, type="response")
testData$YHat1 <- predict(mylogit, testData, type="response")
testData$YHat2 <- predict(SVMModel, testData, type="response")
testData$YHat3 <- predict(RandomForestModel, testData, type="response")
# These are threshold parameter setting controls.
Predict <- function(t) ifelse(LogisticModel > t , 1,0) #t is the threshold for which the confusion
Predict2 <- function(t) ifelse(SVMResult > t , 1,0) #t is the threshold for which the confusion
Predict3 <- function(t) ifelse(RFResult > t , 1,0) #t is the threshold for which the confusion
confusionMatrix(Predict(0.5), testData$Class) # Logistic Regression
confusionMatrix(Predict2(0.5), testData$Class) # SVM
confusionMatrix(Predict3(0.5), testData$Class) # Random Forest
pred1 <- prediction(testData$YHat1, testData$Class)
pred2 <- prediction(testData$YHat2, testData$Class)
pred3 <- prediction(testData$YHat3, testData$Class)
perf <- performance(pred1, "tpr", "fpr" )
perf2 <- performance(pred2, "tpr", "fpr")
perf3 <- performance(pred3, "tpr", "fpr")
plot.new()
plot(perf, col = "green", lwd = 2.5)
plot(perf2, add = TRUE, col = "blue", lwd = 2.5)
plot(perf3, add = TRUE, col = "orange", lwd = 2.5)
abline(0,1,col="Red", lwd=2.5, lty = 2)
title('ROC Curve')
legend(0.8,0.4,c("Logistic","SVM","RF"),
lty=c(1,1,1), # gives the legend appropriate symbols (lines)
lwd=c(1.5,1.5,1.5),col=c("green","blue", "orange")) # gives the legend lines the correct color and width
pred1 <- prediction(testData$YHat1, testData$Class)
pred2 <- prediction(testData$YHat2, testData$Class)
pred3 <- prediction(testData$YHat3, testData$Class)
perf <- performance(pred1, "tpr", "fpr" )
perf2 <- performance(pred2, "tpr", "fpr")
perf3 <- performance(pred3, "tpr", "fpr")
plot.new()
plot(perf, col = "green", lwd = 2.5)
plot(perf2, add = TRUE, col = "blue", lwd = 2.5)
plot(perf3, add = TRUE, col = "orange", lwd = 2.5)
abline(0,1,col="Red", lwd=2.5, lty = 2)
title('ROC Curve')
legend(0.8,0.4,c("Logistic","SVM","RF"),
lty=c(1,1,1), # gives the legend appropriate symbols (lines)
lwd=c(1.5,1.5,1.5),col=c("green","blue", "orange")) # gives the legend lines the correct color and width
pred1 <- prediction(testData$YHat1, testData$Class)
pred2 <- prediction(testData$YHat2, testData$Class)
pred3 <- prediction(testData$YHat3, testData$Class)
perf <- performance(pred1, "tpr", "fpr" )
perf2 <- performance(pred2, "tpr", "fpr")
perf3 <- performance(pred3, "tpr", "fpr")
plot.new()
plot(perf, col = "green", lwd = 2.5)
plot(perf2, add = TRUE, col = "blue", lwd = 2.5)
plot(perf3, add = TRUE, col = "orange", lwd = 2.5)
abline(0,1,col="Red", lwd=2.5, lty = 2)
title('ROC Curve')
legend(0.8,0.4,c("Logistic","SVM","RF"),
lty=c(1,1,1), # gives the legend appropriate symbols (lines)
lwd=c(1.5,1.5,1.5),col=c("green","blue", "orange")) # gives the legend lines the correct color and width
plot.new()
plot(perf, col = "green", lwd = 2.5)
plot(perf2, add = TRUE, col = "blue", lwd = 2.5)
plot(perf3, add = TRUE, col = "orange", lwd = 2.5)
abline(0,1,col="Red", lwd=2.5, lty = 2)
title('ROC Curve')
legend(0.8,0.4,c("Logistic","SVM","RF"),
lty=c(1,1,1), # gives the legend appropriate symbols (lines)
lwd=c(1.5,1.5,1.5),col=c("green","blue", "orange")) # gives the legend lines the correct color and width
View(testData)
View(testData)
